{
  "metadata": {
    "title": "Mip-Splatting: Alias-free 3D Gaussian Splatting",
    "authors": [
      "Universityof"
    ],
    "year": null,
    "doi": null,
    "abstract": null
  },
  "pages": 19,
  "sections": [
    {
      "title": "Faithful Faithful",
      "content": ""
    },
    {
      "title": "Rendering Dilated Rendering",
      "content": "2D Gaussian 2D Gaussian\nImage Plane 5 Pixels\n(Screen Space)"
    },
    {
      "title": "Camera Center",
      "content": "(c) Zoom-out of (a) (d) Zoom-in of (b)"
    },
    {
      "title": "Erosion",
      "content": "(Brake cable\ntoo thin)\nHigh frequency\nartifacts due to\n(S s p cr o e k e e n s D s to i p l o a a c t t e i h o i d c n k il a d t u io e n t ) o Brightening D Fo e c c a r l e l a e s n e g d th I F n o c c r a e l a l s e e n d gth de 3 g D e n G e a r u a s te si a (t n h s in)\nFigure1. 3DGaussianSplatting[18]rendersimagesbyrepresenting3DObjectsas3DGaussianswhichareprojectedontotheimage\nplanefollowedby2DDilationinscreenspaceasshownin(a). Themethod’sintrinsicshrinkagebiasleadstodegenerate3DGau"
    },
    {
      "title": "2.RelatedWork",
      "content": "Novel View Synthesis: NVS is the process of generating\nnew images from viewpoints different from those of the\noriginalcaptures[12,22]. NeRF[28],whichleveragesvol-\numerendering[10,21,25,26],hasbecomeastandardtech-\nniqueinthefield. NeRFutilizesMLPs[5,27,34]tomodel\nscenes as continuous functions, which, despite their com-\npactrepresentation, impederenderingspeedduetotheex-\npensiveMLPevaluationthatisrequiredforeachraypoint.\nSubsequentmethods[16,40,41,52,54]distillapretrained\nNeRF into a sparse repre"
    },
    {
      "title": "3.Preliminaries",
      "content": "paper, we study the out-of-distribution generalization of\n3DGS, training models at a single scale and evaluating it\nInthissection,wefirstreviewthesamplingtheoreminSec-\nacrossmultiplescales.\ntion3.1, layingthefoundationforunderstandingthealias-\nPrimitive-based Differentiable Rendering: Primitive- ing problem. Subsequently, we introduce 3D Gaussian\nbased rendering techniques, which rasterize geometric Splatting (3DGS) [18] and its rendering process in Sec-\nprimitives onto the image plane, have bee"
    },
    {
      "title": "5.MipGaussianSplatting",
      "content": "optimizedusingamulti-viewloss.Duringoptimization,3D\nGaussiansareadaptivelyaddedanddeletedtobetterrepre- To overcome these challenges, we make two modfications\nsentthescene. Wereferthereaderto[18]fordetails. to the original 3DGS model. In particular, we introduce a\n3Dsmoothingfilterthatlimitsthefrequencyofthe3Drep-"
    },
    {
      "title": "4.SensitivitytoSamplingRate resentationtobelowhalfthemaximumsamplingratedeter-",
      "content": "mined by the training images, eliminating high frequency\nIntraditionalforwardsplatting,thecentersp andcolorsc\nk k artifactswhenzoomingin. Moreover,wedemonstratethat\nof Gaussian primitives are predetermined, whereas the 3D\nreplacing 2D screen space dilation with a 2D Mip filter\nGaussian covariance Σ are chosen empirically [42, 59].\nk which approximates the box filter inherent to the physical\nIn contrast, 3DGS [18], optimizes all parameters jointly\nimaging process and effectively mitigates aliasin"
    },
    {
      "title": "Gaussiandoesnotexceedhalfofitsmaximalsamplingrate",
      "content": "the image resolution, camera focal length, and the scene’s\nfor at least one camera. Note that G becomes an intrin-\ndistance from the camera. For an image with focal length low\nsic part of the 3D representation, remaining constant post-\nf inpixelunits, thesamplingintervalinscreenspaceis1.\ntraining.\nWhenthis pixelintervalis back-projected tothe3D world\nspace, it results in a world space sampling interval Tˆ at a 5.2.2DMipFilter\ngivendepthd,withsamplingfrequencyνˆasitsinverse:\nWhile our 3D smoothin"
    },
    {
      "title": "6.Experiments benchmark for this setting, we trained all baseline meth-",
      "content": "odsourselves. WeuseNeRFAcc[23]’simplementationfor\nWe first present the implementation details of Mip-\nNeRF[28],Instant-NGP[32],andTensoRF[4]foritseffi-\nSplatting. We then assess its performance on the Blender\nciency. Official implementations were employed for Mip-\ndataset[28]andthechallengingMip-NeRF360dataset[2].\nNeRF [1], Tri-MipRF [17], and 3DGS [18]. The quan-\nFinally,wediscussthelimitationsofourapproach.\ntitative results, as presented in Table 2, indicate that our\n6.1.Implementation method "
    },
    {
      "title": "7.Conclusion",
      "content": "NeRF++[56] 25.11 0.676 0.375\nPlenoxels[11] 23.08 0.626 0.463\nInstantNGP [32,52] 25.68 0.705 0.302 WepresentedMip-Splatting,amodificationto3DGaussian\nmip-NeRF360[2,30] 27.57 0.793 0.234 Splatting, whichintroducestwonovelfilters, namelya3D\nZip-NeRF[3] 28.54 0.828 0.189 smoothing filter and a 2D Mip filter, to achieve alias-free\n3DGS[18] 27.21 0.815 0.214\nrenderingatarbitraryscales. Our3Dsmoothingfiltereffec-\n3DGS[18]* 27.70 0.826 0.202\n3DGS[18]+EWA[59] 27.77 0.826 0.206 tivelylimitsthemaximalfrequ"
    },
    {
      "title": "Supplementary Material",
      "content": "Inthissupplementarydocument,wefirstpresentabla- dataset[2]. Herewepresentanadditionexperimentevalu-\ntionstudiesofMip-SplattinginSection8. Next,wereport atingbothzoom-outandzoom-ineffectsontheMip-NeRF\nadditionalquantitativeandqualityresultsinSection9. 360 dataset [2]. We use the images downsampled by a\nfactor of 4 for training and evaluate it at multiple resolu-"
    },
    {
      "title": "8.Ablation tions (1/4×, 1/2×, 1×, 2×, 4×). The quantitative results",
      "content": "are presented in Table 7 and the qualitative comparison is\nIn this section, we evaluate the effectiveness of our 3D\nshowninFigure7. Mip-Splattingsignificantlyoutperforms\nsmoothingfilterand2DMipfilterinSection8.1and Sec-\n3DGS [18] and 3DGS + EWA [59] in rendering quality\ntion8.2. Then,wepresentanadditionalexperimenttoeval-\nwhen zooming in and out, which is consistent to our main\nuatebothzoom-inandzoom-outeffectsinthesamedataset\nresults. Further,removingour3Dsmoothingfilterleadsto\ninSection8.3.\nhi"
    },
    {
      "title": "PSNR",
      "content": "chair drums ficus hotdog lego materials mic ship Average\nNeRFw/oL [1,28] 29.92 23.27 27.15 32.00 27.75 26.30 28.40 26.46 27.66\narea\nNeRF[28] 33.39 25.87 30.37 35.64 31.65 30.18 32.60 30.09 31.23\nMipNeRF[1] 37.14 27.02 33.19 39.31 35.74 32.56 38.04 33.08 34.51\nPlenoxels[11] 32.79 25.25 30.28 34.65 31.26 28.33 31.53 28.59 30.34\nTensoRF[4] 32.47 25.37 31.16 34.96 31.73 28.53 31.48 29.08 30.60\nInstant-ngp[32] 32.95 26.43 30.41 35.87 31.83 29.31 32.58 30.23 31.20\nTri-MipRF[17]* 37.67 27.35 33.57 38.7"
    },
    {
      "title": "SSIM",
      "content": "chair drums ficus hotdog lego materials mic ship Average\nNeRFw/oL [1,28] 0.944 0.891 0.942 0.959 0.926 0.934 0.958 0.861 0.927\narea\nNeRF[28] 0.971 0.932 0.971 0.979 0.965 0.967 0.980 0.900 0.958\nMipNeRF[1] 0.988 0.945 0.984 0.988 0.984 0.977 0.993 0.922 0.973\nPlenoxels[11] 0.968 0.929 0.972 0.976 0.964 0.959 0.979 0.892 0.955\nTensoRF[4] 0.967 0.930 0.974 0.977 0.967 0.957 0.978 0.895 0.956\nInstant-ngp[32] 0.971 0.940 0.973 0.979 0.966 0.959 0.981 0.904 0.959\nTri-MipRF[17]* 0.990 0.951 0.985 0.98"
    },
    {
      "title": "LPIPS",
      "content": "chair drums ficus hotdog lego materials mic ship Average\nNeRFw/oL [1,28] 0.035 0.069 0.032 0.028 0.041 0.045 0.031 0.095 0.052\narea\nNeRF[28] 0.028 0.059 0.026 0.024 0.035 0.033 0.025 0.085 0.044\nMipNeRF[1] 0.011 0.044 0.014 0.012 0.013 0.019 0.007 0.062 0.026\nPlenoxels[11] 0.040 0.070 0.032 0.037 0.038 0.055 0.036 0.104 0.051\nTensoRF[4] 0.042 0.075 0.032 0.035 0.036 0.063 0.040 0.112 0.054\nInstant-ngp[32] 0.035 0.066 0.029 0.028 0.040 0.051 0.032 0.095 0.047\nTri-MipRF[17]* 0.011 0.046 0.016 0.01"
    },
    {
      "title": "PSNR",
      "content": "chair drums ficus hotdog lego materials mic ship Average\nNeRF[28] 31.99 25.31 30.74 34.45 30.69 28.86 31.41 28.36 30.23\nMipNeRF[1] 32.89 25.58 31.80 35.40 32.24 29.46 33.26 29.88 31.31\nTensoRF[4] 32.17 25.51 31.19 34.69 31.46 28.60 31.50 28.71 30.48\nInstant-ngp[32] 32.18 25.05 31.32 34.85 31.53 28.59 32.15 28.84 30.57\nTri-MipRF[17] 32.48 24.01 28.41 34.45 30.41 27.82 31.19 27.02 29.47\n3DGS[18] 26.81 21.17 26.02 28.80 25.36 23.10 24.39 23.05 24.84\n3DGS[18]+EWA[59] 32.85 24.91 31.94 33.33 29.76 27"
    },
    {
      "title": "SSIM",
      "content": "chair drums ficus hotdog lego materials mic ship Average\nNeRF[28] 0.968 0.936 0.976 0.977 0.963 0.964 0.980 0.887 0.956\nMipNeRF[1] 0.974 0.939 0.981 0.982 0.973 0.969 0.987 0.915 0.965\nTensoRF[4] 0.970 0.938 0.978 0.979 0.970 0.963 0.981 0.906 0.961\nInstant-ngp[32] 0.970 0.935 0.977 0.980 0.969 0.962 0.982 0.909 0.961\nTri-MipRF[17] 0.971 0.908 0.957 0.975 0.957 0.953 0.975 0.883 0.947\n3DGS[18] 0.915 0.851 0.921 0.930 0.882 0.882 0.909 0.827 0.890\n3DGS[18]+EWA[59] 0.978 0.942 0.983 0.977 0.964 0."
    },
    {
      "title": "LPIPS",
      "content": "chair drums ficus hotdog lego materials mic ship Average\nNeRF[28] 0.040 0.067 0.027 0.034 0.043 0.049 0.035 0.132 0.053\nMipNeRF[1] 0.033 0.062 0.022 0.025 0.030 0.041 0.023 0.092 0.041\nTensoRF[4] 0.036 0.066 0.027 0.030 0.035 0.052 0.034 0.102 0.048\nInstant-ngp[32] 0.036 0.074 0.035 0.030 0.035 0.054 0.034 0.096 0.049\nTri-MipRF[17] 0.026 0.086 0.041 0.023 0.036 0.048 0.023 0.117 0.050\n3DGS[18] 0.047 0.087 0.055 0.034 0.064 0.055 0.046 0.113 0.063\n3DGS[18]+EWA[59] 0.023 0.051 0.017 0.018 0.033 0."
    },
    {
      "title": "PSNR",
      "content": "bicycle flowers garden stump treehill room counter kitchen bonsai\nNeRF[9,28] 21.76 19.40 23.11 21.73 21.28 28.56 25.67 26.31 26.81\nmip-NeRF[1] 21.69 19.31 23.16 23.10 21.21 28.73 25.59 26.47 27.13\nNeRF++[56] 22.64 20.31 24.32 24.34 22.20 28.87 26.38 27.80 29.15\nPlenoxels[11] 21.91 20.10 23.49 20.661 22.25 27.59 23.62 23.42 24.67\nInstantNGP [32,52] 22.79 19.19 25.26 24.80 22.46 30.31 26.21 29.00 31.08\nmip-NeRF360[2,30] 24.40 21.64 26.94 26.36 22.81 31.40 29.44 32.02 33.11\nZip-NeRF[3] 25.80 22.40 "
    },
    {
      "title": "SSIM",
      "content": "bicycle flowers garden stump treehill room counter kitchen bonsai\nNeRF[9,28] 0.455 0.376 0.546 0.453 0.459 0.843 0.775 0.749 0.792\nmip-NeRF[1] 0.454 0.373 0.543 0.517 0.466 0.851 0.779 0.745 0.818\nNeRF++[56] 0.526 0.453 0.635 0.594 0.530 0.852 0.802 0.816 0.876\nPlenoxels[11] 0.496 0.431 0.606 0.523 0.509 0.842 0.759 0.648 0.814\nInstantNGP [32,52] 0.540 0.378 0.709 0.654 0.547 0.893 0.845 0.857 0.924\nmip-NeRF360[2,30] 0.693 0.583 0.816 0.746 0.632 0.913 0.895 0.920 0.939\nZip-NeRF[3] 0.769 0.642 0"
    },
    {
      "title": "LPIPS",
      "content": "bicycle flowers garden stump treehill room counter kitchen bonsai\nNeRF[9,28] 0.536 0.529 0.415 0.551 0.546 0.353 0.394 0.335 0.398\nmip-NeRF[1] 0.541 0.535 0.422 0.490 0.538 0.346 0.390 0.336 0.370\nNeRF++[56] 0.455 0.466 0.331 0.416 0.466 0.335 0.351 0.260 0.291\nPlenoxels[11] 0.506 0.521 0.3864 0.503 0.540 0.419 0.441 0.447 0.398\nInstantNGP [32,52] 0.398 0.441 0.255 0.339 0.420 0.242 0.255 0.170 0.198\nmip-NeRF360[2,30] 0.289 0.345 0.164 0.254 0.338 0.211 0.203 0.126 0.177\nZip-NeRF[3] 0.208 0.273 "
    },
    {
      "title": "PSNR",
      "content": "bicycle flowers garden stump treehill room counter kitchen bonsai\nInstant-NGP[32] 22.51 20.25 24.65 23.15 22.24 29.48 26.18 27.10 29.66\nmip-NeRF360[2] 24.21 21.60 25.82 25.59 22.78 22.95 27.72 28.78 31.63\nzip-NeRF[3] 23.05 20.05 18.07 23.94 22.53 20.51 26.08 27.37 30.05\n3DGS[18] 21.34 19.43 21.94 22.63 20.91 28.10 25.33 23.68 25.89\n3DGS[18]+EWA[59] 23.74 20.94 24.69 24.81 21.93 29.80 27.23 27.07 28.63\nMip-Splatting(ours) 25.26 22.02 26.78 26.65 22.92 31.56 28.87 30.73 31.49"
    },
    {
      "title": "SSIM",
      "content": "bicycle flowers garden stump treehill room counter kitchen bonsai\nInstant-NGP[32] 0.538 0.473 0.647 0.590 0.544 0.868 0.795 0.764 0.877\nmip-NeRF360[2] 0.662 0.567 0.716 0.715 0.628 0.795 0.845 0.828 0.910\nzip-NeRF[3] 0.640 0.521 0.548 0.661 0.590 0.655 0.784 0.800 0.865\n3DGS[18] 0.638 0.536 0.675 0.662 0.591 0.878 0.826 0.789 0.838\n3DGS[18]+EWA[59] 0.671 0.563 0.718 0.693 0.608 0.889 0.843 0.813 0.874\nMip-Splatting(ours) 0.738 0.613 0.786 0.776 0.659 0.921 0.897 0.903 0.933"
    },
    {
      "title": "LPIPS",
      "content": "bicycle flowers garden stump treehill room counter kitchen bonsai\nInstant-NGP[32] 0.500 0.486 0.372 0.469 0.511 0.270 0.310 0.286 0.229\nmip-NeRF360[2] 0.358 0.400 0.296 0.333 0.391 0.256 0.228 0.210 0.182\nzip-NeRF[3] 0.353 0.397 0.346 0.349 0.366 0.302 0.277 0.232 0.236\n3DGS[18] 0.336 0.406 0.295 0.353 0.406 0.223 0.239 0.245 0.242\n3DGS[18]+EWA[59] 0.322 0.395 0.281 0.334 0.405 0.217 0.231 0.216 0.227\nMip-Splatting(ours) 0.281 0.373 0.233 0.281 0.369 0.193 0.199 0.165 0.176\nTable11. Single-scale"
    }
  ],
  "formulas": [
    "k = 1,··· ,K}and\nk\nfiltersizeisfullydeterminedbythetrainingimagesnotthe\nrender an image using volume splatting. The geometry of\nimages to be rendered. While our 2D Mip filter is also a\neach scaled 3D Gaussian G is parameterized by an opac-\nk\nGaussianlowpassfilterinscreenspace,itapproximatesthe ity (scale) α ∈ [0,1], center p ∈ R3×1 and covariance\nk k\nbox filter of the physical imaging process, approximating matrixΣ ∈R3×3definedinworldspace:\nk\na single pixel. Conversely, the EWA filter limits the fre-\nquency signal’s bandwidth to the rendered image, and the G k (x)",
    "k=1 j",
    "g = |Σ | + Σ k s | ·I| e− 2 1(x−pk)T(Σk+ νˆ s k ·I)−1(x−pk)\nthesamplingratedefinedbythetrainingviews. Following k νˆk\nNyquist’s theorem 3.1, we aim to constrain the maximum (9)\nfrequencyofthe3Drepresentationduringoptimization. Here,sisascalarhyperparametertocontrolthesizeofthe\nfilter. Notethatthescale s ofthe3Dfiltersforeachprim-\nMultiviewFrequencyBounds: Multi-viewimagesare2D νˆk\nitive are different as they depend on the training views in\nprojections of a continuous 3D scene. The discrete image\nwhichtheyarevisible.Byemploying3DGaussiansmooth-\ngriddetermineswherewesamplepointsfromthecontinu-\ning,weensurethatthehighestfrequencycomponentofany\nous3Dsignal. Thissamplingrateisintrinsicallyrelatedto\nGaussiandoesnotexceedhalfofitsmaximalsamplingrate\nthe image resolution, camera focal length, and the scene’s\nfor at least one camera. Note that G becomes an intrin-\ndistance from the camera. For an image with focal length low\nsic part of the 3D representation, remaining constant post-\nf inpixelunits, thesamplingintervalinscreenspaceis1.\ntraining.\nWhenthis pixelintervalis back-projected tothe3D world\nspace, it results in a world space sampling interval Tˆ at a 5.2.2DMipFilter\ngivendepthd,withsamplingfrequencyνˆasitsinverse:\nWhile our 3D smoothing filter effectively mitigates high-\nTˆ ",
    "p = |Σ2 k D k +sI| e−1 2 (x−pk)T(Σ2 k D+sI)−1(x−pk)\n(10)\n(cid:32)(cid:26) f (cid:27)N (cid:33) wheresischosentocoverasinglepixelinscreenspace.\nνˆ ",
    "n=1\nfilter[59],theirunderlyingprinciplesaredistinct. OurMip\nwhere N is the total number of images, 1 (p) is an indi- filter is designed to replicate the box filter in the imaging\nn\ncator function that assesses the visibility of a primitive. It process,targetinganexactapproximationofasinglepixel.\nistrueiftheGaussiancenterp fallswithintheviewfrus- Conversely, the EWA filter’s role is to limit the frequency\nk\ntumofthen-thcamera. Intuitively,wechoosethesampling signal’sbandwidth,andthesizeofthefilterischosenempir-\nrate such that there exists at least one camera that is able ically. TheEWApaper[15,59]evenadvocatesforaniden-\nto reconstruct the respective primitive. This process is il- titycovariancematrix,effectivelyoccupyinga3x3pixelre-\nlustrated in Figure 3 for N ",
    "m=100iterations. Wechoose exhibits dilation artifacts. EWA splatting [59] uses a large\nthe variance of our 2D Mip filter as 0.1, approximating a lowpassfiltertolimitthefrequencyoftherenderedimages,\nsinglepixel,andthevarianceofour3Dsmoothingfilteras resultinginoversmoothedimages,whichbecomesparticu-\n0.2,totaling0.3forafaircomparisonwith3DGS[18]and larlyapparentatlowerresolutions.\n3DGS + EWA [59] which replaces the dilation of 3DGS\nwiththeEWAfilter. 6.3.EvaluationontheMip-NeRF360Dataset\n6.2.EvaluationontheBlenderDataset Single-scaleTrainingandMulti-scaleTesting: Tosimu-\nlatezoom-ineffects,wetrainmodelsondatadownsampled\nMulti-scaleTrainingandMulti-scaleTesting: Following by a factor of 8 and rendered at successively higher reso-\npreviouswork[1,17],wetrainourmodelwithmulti-scale lutions (1×, 2×, 4×, and 8×). In the absence of a public\ndata and evaluate on multi-scale data. Similar to [1, 17] benchmark for this setting, we trained all baseline meth-\nwhereraysoffullresolutionimagesaresampledmorefre- odsourselves. WeusetheofficialimplementationforMip-\nquently compared to lower resolution images, we sample NeRF 360 [1] and 3DGS [18] and use a community reim-\n40 percent of full resolution images and 20 percent from plementation for Zip-NeRF [3]4 as the code is not avail-\nother image resolutions each. Our quantitative evaluation able. TheresultsinTable3showthatourmethodperforms\nis shown in Table 1. Our approach attains comparable or comparabletopriorworkatthetrainingscale(1×)andsig-\nsuperiorperformancecomparedtostate-of-the-artmethods nificantlyexceedsallstate-of-the-artmethodsathigherres-\nsuch as Mip-NeRF [1] and Tri-MipRF [17]. Notably, our olutions. As depicted in Figure 5, our method generates\nmethodoutperforms3DGS[18]and3DGS+EWA[59]by highfidelityimagerywithouthigh-frequencyartifacts. No-\nasubstantialmargin,owingtoits2DMipfilter. tably, both Mip-NeRF 360 [2] and Zip-NeRF [3] exhibit\nSingle-scale Training and Multi-scale Testing: Contrary subpar performance at increased resolutions, likely due to\ntopriorworkthatevaluatesmodelstrainedonsingle-scale their MLPs’ inability to extrapolate to out-of-distribution\ndata at the same scale, we consider the an important new frequencies. While 3DGS [18] introduces notable erosion\n3https://github.com/graphdeco-inria/gaussian-splatting 4https://github.com/SuLvXiangXin/zipnerf-pytorch\n6\n\nlluF\n2/1\n4/1\n8/1\nlluF\n2/1\n4/1\n8/1\nlluF\n2/1\n4/1\n8/1\nMip-NeRF[1] Tri-MipRF[17] 3DGS[18] 3DGS[18]+EWA[59] Mip-Splatting(ours) GT\nFigure4. Single-scaleTrainingandMulti-scaleTestingontheBlenderDataset[28]. Allmethodsaretrainedatfullresolutionand\nevaluatedatdifferent(smaller)resolutionstomimiczoom-out.Methodsbasedon3DGScapturefinedetailsbetterthanMip-NeRF[1]and\nTri-MipRF[17]attrainingresolution.Mip-Splattingsurpassesboth3DGS[18]and3DGS+EWA[59]atlowerresolutions.\nPSNR↑ SSIM↑ LPIPS↓\nFullRes.1/2Res.1/4Res.1/8Res. Avg. FullRes.1/2Res.1/4Res.1/8Res. Avg. FullRes.1/2Res.1/4Res.1/8Res Avg.\nNeRF[28] 31.48 32.43 30.29 26.70 30.23 0.949 0.962 0.964 0.951 0.956 0.061 0.041 0.044 0.067 0.053\nMipNeRF[1] 33.08 33.31 30.91 27.97 31.31 0.961 0.970 0.969 0.961 0.965 0.045 0.031 0.036 0.052 0.041\nTensoRF[4] 32.53 32.91 30.01 26.45 30.48 0.960 0.969 0.965 0.948 0.961 0.044 0.031 0.044 0.073 0.048\nInstant-NGP[32] 33.09 33.00 29.84 26.33 30.57 0.962 0.969 0.964 0.947 0.961 0.044 0.033 0.046 0.075 0.049\nTri-MipRF[17] 32.89 32.84 28.29 23.87 29.47 0.958 0.967 0.951 0.913 0.947 0.046 0.033 0.046 0.075 0.050\n3DGS[18] 33.33 26.95 21.38 17.69 24.84 0.969 0.949 0.875 0.766 0.890 0.030 0.032 0.066 0.121 0.063\n3DGS[18]+EWA[59] 33.51 31.66 27.82 24.63 29.40 0.969 0.971 0.959 0.940 0.960 0.032 0.024 0.033 0.047 0.034\nMip-Splatting(ours) 33.36 34.00 31.85 28.67 31.97 0.969 0.977 0.978 0.973 0.974 0.031 0.019 0.019 0.026 0.024\nTable2. Single-scaleTrainingandMulti-scaleTestingontheBlenderDataset[28]. Allmethodsaretrainedonfull-resolutionimages\nand evaluated at four different (smaller) resolutions, with lower resolutions simulating zoom-out effects. While Mip-Splatting yields\ncomparableresultsattrainingresolution,itsignificantlysurpassespreviousworkatallotherscales.\nartifactsduetodilationoperations,3DGS+EWA[59]per- firmsourmethod’seffectivenesstohandlevarioussettings.\nformsbetterwhilestillyieldingpronouncedhigh-frequency\nartifacts. In contrast, our method avoids such artifacts, 6.4.Limitations\nyieldingaestheticallypleasingimagesthatmorecloselyre-\nsemblegroundtruth.It’simportanttoremarkthatrendering OurmethodemploysaGaussianfilterasanapproximation\nathigherresolutionsisasuper-resolutiontask,andmodels to a box filter for efficiency. However, this approximation\nshould not hallucinate high-frequency details absent from introduces errors, particularly when the Gaussian is small\nthetrainingdata. in screen space. This issue correlates with our experimen-\ntal findings, where increased zooming out leads to larger\nSingle-scaleTrainingandSame-scaleTesting:Wefurther errors, as evidenced in Table 2. Additionally, there is a\nevaluateourmethodontheMip-NeRF360dataset[2]fol- slightincreaseintrainingoverheadasthesamplingratefor\nlowing the widely used setting, where models are trained each 3D Gaussian must be calculated every m "
  ],
  "code_blocks": [],
  "tables": 37,
  "full_text": "Mip-Splatting: Alias-free 3D Gaussian Splatting\nZehaoYu1,2 AnpeiChen1,2 BinbinHuang3 TorstenSattler4 AndreasGeiger1,2\n1UniversityofTu¨bingen 2Tu¨bingenAICenter 3ShanghaiTechUniversity\n4CzechTechnicalUniversityinPrague\nhttps://niujinshuchong.github.io/mip-splatting\n(a) Faithful Representation (b) Degenerate Representation\n3D Object\n3D Gaussian\nFaithful Faithful\nRendering Dilated Rendering\n2D Gaussian 2D Gaussian\nImage Plane 5 Pixels\n(Screen Space)\nCamera Center\n(c) Zoom-out of (a) (d) Zoom-in of (b)\nErosion\n(Brake cable\ntoo thin)\nHigh frequency\nartifacts due to\n(S s p cr o e k e e n s D s to i p l o a a c t t e i h o i d c n k il a d t u io e n t ) o Brightening D Fo e c c a r l e l a e s n e g d th I F n o c c r a e l a l s e e n d gth de 3 g D e n G e a r u a s te si a (t n h s in)\nFigure1. 3DGaussianSplatting[18]rendersimagesbyrepresenting3DObjectsas3DGaussianswhichareprojectedontotheimage\nplanefollowedby2DDilationinscreenspaceasshownin(a). Themethod’sintrinsicshrinkagebiasleadstodegenerate3DGaussians\nexceedsamplinglimitasillustratedbytheδfunctionin(b)whilerenderingsimilarlyto2Dduetothedilationoperation. However,when\nchangingthesamplingrate(viathefocallengthorcameradistance),weobservestrongdilationeffects(c)andhighfrequencyartifacts(d).\nAbstract 1.Introduction\nRecently, 3D Gaussian Splatting has demonstrated im-\npressivenovelviewsynthesisresults,reachinghighfidelity Novel View Synthesis (NVS) plays a critical role in com-\nand efficiency. However, strong artifacts can be observed puter graphics and computer vision, with various appli-\nwhen changing the sampling rate, e.g., by changing focal cations including virtual reality, cinematography, robotics,\nlengthorcameradistance. Wefindthatthesourceforthis and more. A particularly significant advancement in this\nphenomenon can be attributed to the lack of 3D frequency fieldistheNeuralRadianceField(NeRF)[28],introduced\nconstraintsandtheusageofa2Ddilationfilter. Toaddress by Mildenhall et al. in 2020. NeRF utilizes a multi-\nthis problem, we introduce a 3D smoothing filter which layer perceptron (MLP) to represent geometry and view-\nconstrains the size of the 3D Gaussian primitives based dependent appearance effectively, demonstrating remark-\non the maximal sampling frequency induced by the input able novel view rendering quality. Recently, 3D Gaussian\nviews, eliminating high-frequency artifacts when zooming Splatting(3DGS)[18]hasgainedattentionasanappealing\nin. Moreover, replacing 2D dilation with a 2D Mip filter, alternative to both MLP [28] and feature grid-based repre-\nwhichsimulatesa2Dboxfilter, effectivelymitigatesalias- sentations [4, 11, 24, 32, 46]. 3DGS stands out for its im-\ninganddilationissues.Ourevaluation,includingscenarios pressivenovelviewsynthesisresults,whileachievingreal-\nsuchatrainingonsingle-scaleimagesandtestingonmul- time rendering at high resolutions. This effectiveness and\ntiplescales,validatestheeffectivenessofourapproach. efficiency, coupled with the potential integration into the\nstandardrasterizationpipelineofGPUsrepresentsasignif-\n1\n3202\nvoN\n72\n]VC.sc[\n1v39461.1132:viXra\n\nicantsteptowardspracticalusageofNVSmethods.\nSpecifically, 3DGS represents complex scenes as a set\nof 3D Gaussians, which are rendered to screen space\nthroughsplatting-basedrasterization. Theattributesofeach\n3D Gaussian, i.e., position, size, orientation, opacity, and\ncolor,areoptimizedthroughamulti-viewphotometricloss.\nThereafter, a 2D dilation operation is applied in screen\nspace for low-pass filtering. Although 3DGS has demon-\nstrated impressive NVS results, it produces artifacts when\ncameraviewsdivergefromthoseseenduringtraining,such\nas zoom in and zoom out, as illustrated in Figure 1. We\nfind that the source for this phenomenon can be attributed\nto the lack of 3D frequency constraints and the usage of a\n2D dilation filter. Specifically, zooming out leads to a re-\nduced size of the projected 2D Gaussians in screen space,\nwhileapplyingthesameamountofdilationresultsindila-\ntion artifacts. Conversely, zooming in causes erosion arti-\nfactssincetheprojected2DGaussiansexpand,yetdilation\nremainsconstant,causingerosionandresultinginincorrect\ngapsbetweenGaussiansinthe2Dprojection.\nToresolvetheseissues,weproposetoregularizethe3D\nrepresentationin3Dspace. Ourkeyinsightisthatthehigh-\nest frequency that can be reconstructed of a 3D scene is\ninherently constrained by the sampling rates of the input\nimages. Wefirstderivethemulti-viewfrequencyboundsof\neachGaussianprimitivebasedonthetrainingviewsaccord-\ning to the Nyquist-Shannon Sampling Theorem [33, 45].\nByapplyingalow-passfiltertothe3DGaussianprimitives\nin3Dspaceduringtheoptimization,weeffectivelyrestrict\nthemaximalfrequencyofthe3Drepresentationtomeetthe\nNyquistlimit. Post-training,thisfilterbecomesanintrinsic\npartofthescenerepresentation,remainingconstantregard-\nlessofviewpointchanges. Consequently,ourmethodelim-\ninatestheartifactspresentsin3DGS[18]whenzoomingin,\nasshowninthe8×higherresolutionimageinFigure2.\nNonetheless,renderingthereconstructedsceneatlower\nsampling rates (e.g., zooming out) results in aliasing. Pre-\nvious work [1–3, 17] address aliasing by employing cone\ntracingandapplyingpre-filteringtotheinputpositionalor\nfeature encoding, which is not applicable to 3DGS. Thus,\nwe introduce a 2D Mip filter (a` la “mipmap”) specifically\ndesigned to ensure alias-free reconstruction and render-\ning across different scales. Our 2D Mip filter mimics the\n2D box filter inherent to the actual physical imaging pro-\ncess [29, 37, 48].by approximating it with a 2D Gaussian\nlow pass filter. In contrast to previous work [1–3, 17] that\nrelyontheMLP’sabilitytointerpolatemulti-scalesignals\nduring training with multi-scale images, our closed-form\nmodification to the 3D Gaussian representation results in\nexcellent out-of-distribution generalization: Training at a\nsingle sampling rate enables faithful rendering at various\nsamplingratesdifferentfromthoseusedduringtrainingas\ndemonstratedbythe1/4×down-sampledimageinFigure2.\nnoituloseR×8\nnoituloseRlluF\nnoituloseR4/1\n3DGS [18] 3DGS+EWA[59] Mip-Splatting Reference\nFigure2. We trained allthemodels onsingle-scale(fullresolu-\ntionhere)imagesandrenderedimageswithdifferentresolutions\nby changing focal length. While all methods show similar per-\nformanceattrainingscale,weobservestrongartifactsinprevious\nwork[18,59]whenchangingthesamplingrate. Bycontrast,our\nMip-Splattingrendersfaithfulimagesacrossdifferentscales.\nInsummary,wemakethefollowingcontributions:\n• We introduce a 3D smoothing filter for 3DGS to effec-\ntively regularize the maximum frequency of 3D Gaus-\nsianprimitives,resolvingtheartifactsobservedinout-of-\ndistributionrenderingsofpriormethods[18,59].\n• Wereplacethe2Ddilationfilterwitha2DMipfilterto\naddressaliasinganddilationartifacts.\n• Experiments on challenging benchmark datasets [2, 28]\ndemonstrate the effectiveness of Mip-Splatting when\nmodifyingthesamplingrate.\n• Ourmodificationsto3DGSareprincipledandsimple,re-\nquiringonlyfewchangestotheoriginal3DGScode.\n2.RelatedWork\nNovel View Synthesis: NVS is the process of generating\nnew images from viewpoints different from those of the\noriginalcaptures[12,22]. NeRF[28],whichleveragesvol-\numerendering[10,21,25,26],hasbecomeastandardtech-\nniqueinthefield. NeRFutilizesMLPs[5,27,34]tomodel\nscenes as continuous functions, which, despite their com-\npactrepresentation, impederenderingspeedduetotheex-\npensiveMLPevaluationthatisrequiredforeachraypoint.\nSubsequentmethods[16,40,41,52,54]distillapretrained\nNeRF into a sparse representation, enabling real-time ren-\ndering of NeRFs. Further advancements have been made\nto improve the training and rendering of NeRF with ad-\nvanced scene representations [4, 6, 11, 18, 19, 24, 32, 46,\n51]. In particular, 3D Gaussians Splatting (3DGS) [18]\n2\n\ndemonstratedimpressivenovelviewsynthesisresults,while for feature grid-based representations [3, 17, 58]. How-\nachievingreal-timerenderingathigh-definitionresolutions. ever,theseapproachesrequiremulti-scaleimagesforsuper-\nImportantly,3DGSrepresentsthesceneexplicitlyasacol- vision.Incontrast,ourapproachisbasedon3DGS[18]and\nlection of 3D Gaussians and uses rasterization instead of determinesthenecessarylow-passfiltersizebasedonpixel\nraytracing. Nevertheless,3DGSfocusesonin-distribution size,allowingforalias-freerenderingatscalesunobserved\nevaluationwheretrainingandtestingareconductedatsim- duringtraining.\nilar sampling rates (focal length/scene distance). In this\n3.Preliminaries\npaper, we study the out-of-distribution generalization of\n3DGS, training models at a single scale and evaluating it\nInthissection,wefirstreviewthesamplingtheoreminSec-\nacrossmultiplescales.\ntion3.1, layingthefoundationforunderstandingthealias-\nPrimitive-based Differentiable Rendering: Primitive- ing problem. Subsequently, we introduce 3D Gaussian\nbased rendering techniques, which rasterize geometric Splatting (3DGS) [18] and its rendering process in Sec-\nprimitives onto the image plane, have been explored ex- tion3.2.\ntensively due to their efficiency [13, 14, 38, 44, 59, 60].\n3.1.SamplingTheorem\nDifferentiable point-based rendering methods [20, 36, 39,\n43, 49, 53, 57] offer great flexibility in representing in- The Sampling Theorem, also known as the Nyquist-\ntricate structures and are thus well-suited for novel view ShannonSamplingTheorem[33,45],isafundamentalcon-\nsynthesis. Notably, Pulsar [20] stands out for its efficient ceptinsignalprocessinganddigitalcommunicationthatde-\nsphere rasterization. The more recent 3D Gaussian Splat- scribestheconditionsunderwhichacontinuoussignalcan\nting (3DGS) work [18] utilizes anisotropic Gaussians [59] beaccuratelyrepresentedorreconstructedfromitsdiscrete\nand introduces a tile-based sorting for rendering, achiev- samples.Toaccuratelyreconstructacontinuoussignalfrom\ning remarkable frame rates. Despite its impressive results, itsdiscretesampleswithoutlossofinformation,thefollow-\n3DGS exhibits strong artifacts when rendering at a differ- ingconditionsmustbemet:\nent sampling rate. We address this issue by introducing a\nCondition1 The continuous signal must be band-limited\n3Dsmoothingfiltertoconstrainthemaximalfrequenciesof\nand may not contain any frequency components above a\nthe3DGaussianprimitiverepresentation,anda2DMipfil-\ncertainmaximumfrequencyν.\nterthatapproximatestheboxfilterofthephysicalimaging\nprocessforalias-freerendering. Condition2 Thesamplingrateνˆmustbeatleasttwicethe\nAnti-aliasinginRendering:Therearetwoprincipalstrate- highestfrequencypresentinthecontinuoussignal:νˆ≥2ν.\ngies to combat aliasing: super-sampling, which increases\nInpractice,tosatisfytheconstraintswhenreconstructing\nthe number of samples [7], and prefiltering, which ap-\na signal from discrete samples, a low-pass or anti-aliasing\nplies low-pass filtering to the signal to meet the Nyquist\nfilter is applied to the signal before sampling. The filter\nlimit [8, 15, 31, 47, 50, 59]. For example, EWA splat- eliminatesanyfrequencycomponentsabove νˆ andattenu-\nting[59]appliesaGaussianlowpassfiltertotheprojected 2\nateshigh-frequencycontentthatcouldleadtoaliasing.\n2DGaussianinscreenspacetoproduceabandlimitedout-\nput respecting the Nyquist frequency of the image. While 3.2.3DGaussianSplatting\nwe also apply a band-limited filter to the Gaussian primi-\nPriorworks[18,59]proposetorepresenta3Dsceneasaset\ntives,ourband-limitedfilterisappliedin3Dspaceandthe\nofscaled3DGaussianprimitives{G |k = 1,··· ,K}and\nk\nfiltersizeisfullydeterminedbythetrainingimagesnotthe\nrender an image using volume splatting. The geometry of\nimages to be rendered. While our 2D Mip filter is also a\neach scaled 3D Gaussian G is parameterized by an opac-\nk\nGaussianlowpassfilterinscreenspace,itapproximatesthe ity (scale) α ∈ [0,1], center p ∈ R3×1 and covariance\nk k\nbox filter of the physical imaging process, approximating matrixΣ ∈R3×3definedinworldspace:\nk\na single pixel. Conversely, the EWA filter limits the fre-\nquency signal’s bandwidth to the rendered image, and the G k (x)=e−1 2 (x−pk)TΣ− k 1(x−pk) (1)\nsize of the filter is chosen empirically. A critical differ-\nToconstrainΣ tothespaceofvalidcovariancematrices,a\nence to [59] is that we tackle the reconstruction problem, k\nsemi-definite parameterization Σ = O s sTOT is used.\noptimizingthe3DGaussianrepresentationviainverseren- k k k k k\nHere,s∈R3isascalingvectorandO∈R3×3isarotation\ndering while EWA splatting only considers the rendering\nmatrix,parameterizedbyaquaternion[18].\nproblem.\nTorenderanimageforagivenviewpointdefinedbyro-\nRecent neural rendering methods integrate pre-filtering\ntationR∈R3×3 andtranslationt∈R3,the3DGaussians\nto mitigate aliasing [1–3, 17, 58]. Mip-NeRF [1], for in-\n{G }arefirsttransformedintocameracoordinates:\nstance,introducedanintegratedpositionencoding(IPE)to k\nattenuatehigh-frequencydetails. Asimilarideaisadapted p′ =Rp +t, Σ′ =RΣ RT (2)\nk k k k\n3\n\nAfterwards,theyareprojectedtorayspaceviaalocalaffine 3DGS indeed systematically underestimates the scale pa-\ntransformation rameterof3DGaussiansduringoptimization.\nΣ′′ =J Σ′ JT (3) Whilethisdoesnotaffectrenderingatsimilarsampling\nk k k k\nrates (cf. Figure 1 (a) vs. (b)), it leads to erosion effects\nwheretheJacobianmatrixJ isanaffineapproximationto\nk whenzoominginormovingthecameracloser. Thisisbe-\ntheprojectivetransformationdefinedbythecenterofthe3D\ncause the dilated 2D Gaussians become smaller in screen\nGaussianp′. ByskippingthethirdrowandcolumnofΣ′′,\nk k space. In this case, the rendered image exhibits high-\nweobtaina2DcovariancematrixΣ2Dinrayspace,andwe\nk frequencyartifacts,renderingobjectstructuresthinnerthan\nuseG2D torefertothecorrespondingscaled2DGaussian,\nk theyactuallyappearasillustratedinFigure1(d).\nsee[18]fordetails.\nConversely,screenspacedilationalsonegativelyaffects\nFinally,3DGS[18]utilizessphericalharmonicstomodel\nrendering when decreasing the sampling rate as illustrated\nview-dependent color c and renders image via alpha\nk inFigure1(c)whichshowsazoomed-outversionof(a). In\nblendingaccordingtotheprimitive’sdepthorder1,...,K:\nthiscase,dilationspreadsradianceinaphysicallyincorrect\nway across pixels. Note that in (c), the area covered by\nK k−1\nc(x)= (cid:88) c α G2D(x) (cid:89) (1−α G2D(x)) (4) the projection of the 3D object is smaller than a pixel, yet\nk k k j j\nthe dilated Gaussian is not attenuated, accumulating more\nk=1 j=1\nlight than what physically reaches the pixel. This leads to\nDilation:Toavoiddegeneratecaseswheretheprojected2D increased brightness and dilation artifacts which strongly\ndegradetheappearanceofthebicyclewheels’spokes.\nGaussiansaretoosmallinscreenspace,i.e.,smallerthana\nThe aforementioned scale ambiguity becomes particu-\npixel,theprojected2DGaussiansaredilatedasfollows:\nlarly problematic in representations involving millions of\nG k 2D(x)=e−1 2 (x−pk)T(Σ2 k D+sI)−1(x−pk) (5) Gaussians. However, simply discarding screen space dila-\ntionresultsinoptimizationchallengesforcomplexscenes,\nwhere I is a 2D identity matrix and s is a scalar dilation such as those present in the Mip-NeRF 360 dataset [2],\nhyperparameter. Notethatthisoperatoradjuststhescaleof wherealargenumberofsmallGaussianarecreatedbythe\nthe2DGaussianwhileleavingitsmaximumunchanged.As density control mechanism [18], exceeding GPU capacity.\nthiseffectissimilartothatofdilationoperatorsinmorphol- Moreover,evenifamodelcanbesuccessfullytrainedwith-\nogy,wecalledita2Dscreenspacedilationoperation1. outdilation,decreasingthesamplingrateresultsinaliasing\neffectsduetothelackofanti-aliasing[59].\nReconstruction: As the rendering process is fast and dif-\nferentiable, the 3D Gaussian parameters can be efficiently\n5.MipGaussianSplatting\noptimizedusingamulti-viewloss.Duringoptimization,3D\nGaussiansareadaptivelyaddedanddeletedtobetterrepre- To overcome these challenges, we make two modfications\nsentthescene. Wereferthereaderto[18]fordetails. to the original 3DGS model. In particular, we introduce a\n3Dsmoothingfilterthatlimitsthefrequencyofthe3Drep-\n4.SensitivitytoSamplingRate resentationtobelowhalfthemaximumsamplingratedeter-\nmined by the training images, eliminating high frequency\nIntraditionalforwardsplatting,thecentersp andcolorsc\nk k artifactswhenzoomingin. Moreover,wedemonstratethat\nof Gaussian primitives are predetermined, whereas the 3D\nreplacing 2D screen space dilation with a 2D Mip filter\nGaussian covariance Σ are chosen empirically [42, 59].\nk which approximates the box filter inherent to the physical\nIn contrast, 3DGS [18], optimizes all parameters jointly\nimaging process and effectively mitigates aliasing and di-\nthrough an inverse rendering framework by backpropagat-\nlationissues. Incombination, Mip-Splattingenablesalias-\ningamulti-viewphotometricloss.\nfreerenderings2acrossvarioussamplingrates.Wenowdis-\nWe observe that this optimization suffers from ambigu-\ncussthethe3Dsmoothingandthe2DMipfiltersindetail.\nitiesasillustratedinFigure1whichshowsasimpleexam-\npleinvolvingoneobjectandanimagesensorwith5pixels. 5.1.3DSmoothingFilter\nConsider the 3D object in (a), its approximation by a 3D\n3D radiance field reconstruction from multi-view observa-\nGaussian and its projection into screen space (blue pixel).\ntions is a well-known ill-posed problem as multiple dis-\nDuetoscreenspacedilation(Eq.5)withaGaussiankernel\ntinctly different reconstructions can result in the same 2D\n(size ≈ 1 pixel), the degenerate 3D Gaussian represented\nprojections [2, 55, 56]. Our key insight is that the high-\nbyaDiracδ functionin(b)leadstoasimilarimage. This\nest frequency of a reconstructed 3D scene is limited by\nillustratesthatthescaleofthe3DGaussianisnotproperly\nconstrained. Inpractice, duetoitsimplicitshrinkagebias, 2Notethatweusealiastorefertomultipleartifactsdiscussedinthe\npaper,includingdilation,erosion,oversmoothing,high-frequencyartifacts\n1Thedilationoperationisnotmentionedinoriginalpaper. andaliasingitself.\n4\n\nwerecomputethemaximalsamplingrateofeachGaussian\nprimitiveeverymiterationsaswefoundthe3DGaussians\ncentersremainrelativelystablethroughoutthetraining.\n3DSmoothing: Giventhemaximalsamplingrateνˆ fora\nk\nCamera 4 primitive,weaimtoconstrainthemaximalfrequencyofthe\n(smaller d)\n3Drepresentation. ThisisachievedbyapplyingaGaussian\nCamera 1 Camera 2 Camera 3 low-passfilterG toeach3DGaussianprimitiveG before\n(smaller f) (larger f) low k\nCamera 5 projectingitontoscreenspace:\n(larger d)\nG (x) =(G ⊗G )(x) (8)\nFigure3.Samplinglimits.Apixelcorrespondstosamplinginter- k reg k low\nvalTˆ. Weband-limitthe3DGaussiansbythemaximalsampling\nThisoperationisefficientasconvolvingtwoGaussianswith\nrate(i.e.,minimalsamplinginterval)amongallobservations.This\ncovariancematricesΣ andΣ resultsinanotherGaussian\nexampleshows5camerasatdifferentdepthsdandwithdifferent 1 2\nfocal lengths f. Here, camera 3 determines the minimal Tˆ and withvarianceΣ 1 +Σ 2 . Hence,\nhencethemaximalsamplingrateνˆ. (cid:115)\nG k (x) reg = |Σ | + Σ k s | ·I| e− 2 1(x−pk)T(Σk+ νˆ s k ·I)−1(x−pk)\nthesamplingratedefinedbythetrainingviews. Following k νˆk\nNyquist’s theorem 3.1, we aim to constrain the maximum (9)\nfrequencyofthe3Drepresentationduringoptimization. Here,sisascalarhyperparametertocontrolthesizeofthe\nfilter. Notethatthescale s ofthe3Dfiltersforeachprim-\nMultiviewFrequencyBounds: Multi-viewimagesare2D νˆk\nitive are different as they depend on the training views in\nprojections of a continuous 3D scene. The discrete image\nwhichtheyarevisible.Byemploying3DGaussiansmooth-\ngriddetermineswherewesamplepointsfromthecontinu-\ning,weensurethatthehighestfrequencycomponentofany\nous3Dsignal. Thissamplingrateisintrinsicallyrelatedto\nGaussiandoesnotexceedhalfofitsmaximalsamplingrate\nthe image resolution, camera focal length, and the scene’s\nfor at least one camera. Note that G becomes an intrin-\ndistance from the camera. For an image with focal length low\nsic part of the 3D representation, remaining constant post-\nf inpixelunits, thesamplingintervalinscreenspaceis1.\ntraining.\nWhenthis pixelintervalis back-projected tothe3D world\nspace, it results in a world space sampling interval Tˆ at a 5.2.2DMipFilter\ngivendepthd,withsamplingfrequencyνˆasitsinverse:\nWhile our 3D smoothing filter effectively mitigates high-\nTˆ = 1 = d (6) frequency artifacts [18, 59], rendering the reconstructed\nνˆ f scene at lower sampling rates (e.g., zooming out or mov-\ningthecamerafurtheraway)wouldstillleadtoaliasing. To\nAspositedbyNyquist’stheoremSection3.1,givensamples\novercomethis,wereplacethescreenspacedilationfilterof\ndrawnatfrequencyνˆ,reconstructionalgorithmsareableto\n3DGSbya2DMipfilter.\nreconstructcomponentsofthesignalwithfrequenciesupto\nνˆ, or f . Consequently, a primitive smaller than 2Tˆ may Morespecifically,wereplicatethephysicalimagingpro-\n2 2d cess[29,37,48],wherephotonshittingapixelonthecam-\nresultinaliasingartifactsduringthesplattingprocess,since\nera sensor are integrated over the pixel’s area. While an\nitssizeisbelowtwicethesamplinginterval.\nideal model would use a 2D box filter in image space, we\nTosimplify,weapproximatedepthdusingthecenterof\napproximateitwitha2DGaussianfilterforefficiency\ntheprimitivep ,anddisregardtheimpactofocclusionfor\nk\nsampling interval estimation. Since the sampling rate of a (cid:115)\n|Σ2D|\np d r e i t m er i m tiv in e e is th d e ep m th ax -d im ep a e l n s d a e m n p t l a i n n d g d ra if t f e e f r o s r ac p r r o im ss it c i a v m ek er a a s s,we G k 2D(x) mip = |Σ2 k D k +sI| e−1 2 (x−pk)T(Σ2 k D+sI)−1(x−pk)\n(10)\n(cid:32)(cid:26) f (cid:27)N (cid:33) wheresischosentocoverasinglepixelinscreenspace.\nνˆ =max 1 (p )· n (7)\nk n k d While our Mip filter shares similarities with the EWA\nn n=1\nfilter[59],theirunderlyingprinciplesaredistinct. OurMip\nwhere N is the total number of images, 1 (p) is an indi- filter is designed to replicate the box filter in the imaging\nn\ncator function that assesses the visibility of a primitive. It process,targetinganexactapproximationofasinglepixel.\nistrueiftheGaussiancenterp fallswithintheviewfrus- Conversely, the EWA filter’s role is to limit the frequency\nk\ntumofthen-thcamera. Intuitively,wechoosethesampling signal’sbandwidth,andthesizeofthefilterischosenempir-\nrate such that there exists at least one camera that is able ically. TheEWApaper[15,59]evenadvocatesforaniden-\nto reconstruct the respective primitive. This process is il- titycovariancematrix,effectivelyoccupyinga3x3pixelre-\nlustrated in Figure 3 for N = 5. In our implementation, giononthescreen. However,thisapproachleadstooverly\n5\n\nPSNR↑ SSIM↑ LPIPS↓\nFullRes.1/2Res.1/4Res.1/8Res. Avg. FullRes.1/2Res.1/4Res.1/8Res. Avg. FullRes.1/2Res.1/4Res.1/8Res Avg.\nNeRFw/oL [1,28] 31.20 30.65 26.25 22.53 27.66 0.950 0.956 0.930 0.871 0.927 0.055 0.034 0.043 0.075 0.052\narea\nNeRF[28] 29.90 32.13 33.40 29.47 31.23 0.938 0.959 0.973 0.962 0.958 0.074 0.040 0.024 0.039 0.044\nMipNeRF[1] 32.63 34.34 35.47 35.60 34.51 0.958 0.970 0.979 0.983 0.973 0.047 0.026 0.017 0.012 0.026\nPlenoxels[11] 31.60 32.85 30.26 26.63 30.34 0.956 0.967 0.961 0.936 0.955 0.052 0.032 0.045 0.077 0.051\nTensoRF[4] 32.11 33.03 30.45 26.80 30.60 0.956 0.966 0.962 0.939 0.956 0.056 0.038 0.047 0.076 0.054\nInstant-NGP[32] 30.00 32.15 33.31 29.35 31.20 0.939 0.961 0.974 0.963 0.959 0.079 0.043 0.026 0.040 0.047\nTri-MipRF[17]* 32.65 34.24 35.02 35.53 34.36 0.958 0.971 0.980 0.987 0.974 0.047 0.027 0.018 0.012 0.026\n3DGS[18] 28.79 30.66 31.64 27.98 29.77 0.943 0.962 0.972 0.960 0.960 0.065 0.038 0.025 0.031 0.040\n3DGS[18]+EWA[59] 31.54 33.26 33.78 33.48 33.01 0.961 0.973 0.979 0.983 0.974 0.043 0.026 0.021 0.019 0.027\nMip-Splatting(ours) 32.81 34.49 35.45 35.50 34.56 0.967 0.977 0.983 0.988 0.979 0.035 0.019 0.013 0.010 0.019\nTable1.Multi-scaleTrainingandMulti-scaleTestingontheBlenderdataset[28].Ourapproachachievesstate-of-the-artperformance\ninmostmetrics.Itsignificantlyoutperforms3DGS[18]and3DGS+EWA[59].∗indicatesthatweretrainthemodel.\nsmooth results when zooming out as we will show in our setting that involves training on full-resolution images and\nexperiments. rendering at various resolutions (i.e. 1×, 1/2, 1/4, and 1/8)\nto mimic zoom-out effects. In the absence of a public\n6.Experiments benchmark for this setting, we trained all baseline meth-\nodsourselves. WeuseNeRFAcc[23]’simplementationfor\nWe first present the implementation details of Mip-\nNeRF[28],Instant-NGP[32],andTensoRF[4]foritseffi-\nSplatting. We then assess its performance on the Blender\nciency. Official implementations were employed for Mip-\ndataset[28]andthechallengingMip-NeRF360dataset[2].\nNeRF [1], Tri-MipRF [17], and 3DGS [18]. The quan-\nFinally,wediscussthelimitationsofourapproach.\ntitative results, as presented in Table 2, indicate that our\n6.1.Implementation method significantly outperforms all existing state-of-the-\nart methods. A qualitative comparison is provided in Fig-\nWebuildourmethoduponthepopularopen-source3DGS ure 4. Methods based on 3DGS [18] capture fine details\ncode base [18]3. Following [18], we train our models for more effectively than Mip-NeRF [1] and Tri-MipRF [17],\n30Kiterationsacrossallscenesandusethesamelossfunc- butonlyattheoriginaltrainingscale. Notably,ourmethod\ntion,Gaussiandensitycontrolstrategy,scheduleandhyper- surpasses both 3DGS [18] and 3DGS + EWA [59] in ren-\nparameters. Forefficiency,werecomputethesamplingrate deringqualityatlowerresolutions.Inparticular,3DGS[18]\nofeach3DGaussianeverym=100iterations. Wechoose exhibits dilation artifacts. EWA splatting [59] uses a large\nthe variance of our 2D Mip filter as 0.1, approximating a lowpassfiltertolimitthefrequencyoftherenderedimages,\nsinglepixel,andthevarianceofour3Dsmoothingfilteras resultinginoversmoothedimages,whichbecomesparticu-\n0.2,totaling0.3forafaircomparisonwith3DGS[18]and larlyapparentatlowerresolutions.\n3DGS + EWA [59] which replaces the dilation of 3DGS\nwiththeEWAfilter. 6.3.EvaluationontheMip-NeRF360Dataset\n6.2.EvaluationontheBlenderDataset Single-scaleTrainingandMulti-scaleTesting: Tosimu-\nlatezoom-ineffects,wetrainmodelsondatadownsampled\nMulti-scaleTrainingandMulti-scaleTesting: Following by a factor of 8 and rendered at successively higher reso-\npreviouswork[1,17],wetrainourmodelwithmulti-scale lutions (1×, 2×, 4×, and 8×). In the absence of a public\ndata and evaluate on multi-scale data. Similar to [1, 17] benchmark for this setting, we trained all baseline meth-\nwhereraysoffullresolutionimagesaresampledmorefre- odsourselves. WeusetheofficialimplementationforMip-\nquently compared to lower resolution images, we sample NeRF 360 [1] and 3DGS [18] and use a community reim-\n40 percent of full resolution images and 20 percent from plementation for Zip-NeRF [3]4 as the code is not avail-\nother image resolutions each. Our quantitative evaluation able. TheresultsinTable3showthatourmethodperforms\nis shown in Table 1. Our approach attains comparable or comparabletopriorworkatthetrainingscale(1×)andsig-\nsuperiorperformancecomparedtostate-of-the-artmethods nificantlyexceedsallstate-of-the-artmethodsathigherres-\nsuch as Mip-NeRF [1] and Tri-MipRF [17]. Notably, our olutions. As depicted in Figure 5, our method generates\nmethodoutperforms3DGS[18]and3DGS+EWA[59]by highfidelityimagerywithouthigh-frequencyartifacts. No-\nasubstantialmargin,owingtoits2DMipfilter. tably, both Mip-NeRF 360 [2] and Zip-NeRF [3] exhibit\nSingle-scale Training and Multi-scale Testing: Contrary subpar performance at increased resolutions, likely due to\ntopriorworkthatevaluatesmodelstrainedonsingle-scale their MLPs’ inability to extrapolate to out-of-distribution\ndata at the same scale, we consider the an important new frequencies. While 3DGS [18] introduces notable erosion\n3https://github.com/graphdeco-inria/gaussian-splatting 4https://github.com/SuLvXiangXin/zipnerf-pytorch\n6\n\nlluF\n2/1\n4/1\n8/1\nlluF\n2/1\n4/1\n8/1\nlluF\n2/1\n4/1\n8/1\nMip-NeRF[1] Tri-MipRF[17] 3DGS[18] 3DGS[18]+EWA[59] Mip-Splatting(ours) GT\nFigure4. Single-scaleTrainingandMulti-scaleTestingontheBlenderDataset[28]. Allmethodsaretrainedatfullresolutionand\nevaluatedatdifferent(smaller)resolutionstomimiczoom-out.Methodsbasedon3DGScapturefinedetailsbetterthanMip-NeRF[1]and\nTri-MipRF[17]attrainingresolution.Mip-Splattingsurpassesboth3DGS[18]and3DGS+EWA[59]atlowerresolutions.\nPSNR↑ SSIM↑ LPIPS↓\nFullRes.1/2Res.1/4Res.1/8Res. Avg. FullRes.1/2Res.1/4Res.1/8Res. Avg. FullRes.1/2Res.1/4Res.1/8Res Avg.\nNeRF[28] 31.48 32.43 30.29 26.70 30.23 0.949 0.962 0.964 0.951 0.956 0.061 0.041 0.044 0.067 0.053\nMipNeRF[1] 33.08 33.31 30.91 27.97 31.31 0.961 0.970 0.969 0.961 0.965 0.045 0.031 0.036 0.052 0.041\nTensoRF[4] 32.53 32.91 30.01 26.45 30.48 0.960 0.969 0.965 0.948 0.961 0.044 0.031 0.044 0.073 0.048\nInstant-NGP[32] 33.09 33.00 29.84 26.33 30.57 0.962 0.969 0.964 0.947 0.961 0.044 0.033 0.046 0.075 0.049\nTri-MipRF[17] 32.89 32.84 28.29 23.87 29.47 0.958 0.967 0.951 0.913 0.947 0.046 0.033 0.046 0.075 0.050\n3DGS[18] 33.33 26.95 21.38 17.69 24.84 0.969 0.949 0.875 0.766 0.890 0.030 0.032 0.066 0.121 0.063\n3DGS[18]+EWA[59] 33.51 31.66 27.82 24.63 29.40 0.969 0.971 0.959 0.940 0.960 0.032 0.024 0.033 0.047 0.034\nMip-Splatting(ours) 33.36 34.00 31.85 28.67 31.97 0.969 0.977 0.978 0.973 0.974 0.031 0.019 0.019 0.026 0.024\nTable2. Single-scaleTrainingandMulti-scaleTestingontheBlenderDataset[28]. Allmethodsaretrainedonfull-resolutionimages\nand evaluated at four different (smaller) resolutions, with lower resolutions simulating zoom-out effects. While Mip-Splatting yields\ncomparableresultsattrainingresolution,itsignificantlysurpassespreviousworkatallotherscales.\nartifactsduetodilationoperations,3DGS+EWA[59]per- firmsourmethod’seffectivenesstohandlevarioussettings.\nformsbetterwhilestillyieldingpronouncedhigh-frequency\nartifacts. In contrast, our method avoids such artifacts, 6.4.Limitations\nyieldingaestheticallypleasingimagesthatmorecloselyre-\nsemblegroundtruth.It’simportanttoremarkthatrendering OurmethodemploysaGaussianfilterasanapproximation\nathigherresolutionsisasuper-resolutiontask,andmodels to a box filter for efficiency. However, this approximation\nshould not hallucinate high-frequency details absent from introduces errors, particularly when the Gaussian is small\nthetrainingdata. in screen space. This issue correlates with our experimen-\ntal findings, where increased zooming out leads to larger\nSingle-scaleTrainingandSame-scaleTesting:Wefurther errors, as evidenced in Table 2. Additionally, there is a\nevaluateourmethodontheMip-NeRF360dataset[2]fol- slightincreaseintrainingoverheadasthesamplingratefor\nlowing the widely used setting, where models are trained each 3D Gaussian must be calculated every m = 100 it-\nand tested at the same scale, with indoor scenes down- erations. Currently, this computation is performed using\nsampled by a factor of two and outdoor scenes by four. PyTorch [35] and a more efficient CUDA implementation\nAs shown in Table 4, our method performs on par with could potentially reduce this overhead. Designing a better\n3DGS [18] and 3DGS + EWA [59] in this challenging data structure for precomputing and storing the sampling\nbenchmark,withoutanydecreaseinperformance.Thiscon- rate, as it depends solely on the camera poses and intrin-\n7\n\nMip-NeRF360[2] Zip-NeRF[3] 3DGS[18] 3DGS[18]+EWA[59] Mip-Splatting(ours) GT\nFigure5. Single-scaleTrainingandMulti-scaleTestingontheMip-NeRF360Dataset[2]. Allmodelsaretrainedonimagesdown-\nsampled by a factor of eight and rendered at full resolution to demonstrate zoom-in/moving closer effects. In contrast to prior work,\nMip-Splattingrendersimagesthatcloselyapproximategroundtruth.Pleasealsonotethehigh-frequencyartifactsof3DGS+EWA[59].\nPSNR↑ SSIM↑ LPIPS↓\n1×Res.2×Res.4×Res.8×Res.Avg. 1×Res.2×Res.4×Res.8×Res.Avg. 1×Res.2×Res.4×Res.8×Res. Avg.\nInstant-NGP[32] 26.79 24.76 24.27 24.27 25.02 0.746 0.639 0.626 0.698 0.677 0.239 0.367 0.445 0.475 0.382\nmip-NeRF360[2] 29.26 25.18 24.16 24.10 25.67 0.860 0.727 0.670 0.706 0.741 0.122 0.260 0.370 0.428 0.295\nzip-NeRF[3] 29.66 23.27 20.87 20.27 23.52 0.875 0.696 0.565 0.559 0.674 0.097 0.257 0.421 0.494 0.318\n3DGS[18] 29.19 23.50 20.71 19.59 23.25 0.880 0.740 0.619 0.619 0.715 0.107 0.243 0.394 0.476 0.305\n3DGS[18]+EWA[59] 29.30 25.90 23.70 22.81 25.43 0.880 0.775 0.667 0.643 0.741 0.114 0.236 0.369 0.449 0.292\nMip-Splatting(ours) 29.39 27.39 26.47 26.22 27.37 0.884 0.808 0.754 0.765 0.803 0.108 0.205 0.305 0.392 0.252\nTable3.Single-scaleTrainingandMulti-scaleTestingontheMip-NeRF360Dataset[2].Allmethodsaretrainedonthesmallestscale\n(1×)andevaluatedacrossfourscales(1×,2×,4×,and8×),withevaluationsathighersamplingratessimulatingzoom-ineffects.While\nourmethodyieldscomparableresultsatthetrainingresolution,itsignificantlysurpassesallpreviousworkatallotherscales.\nPSNR↑ SSIM↑ LPIPS↓ additionaloverheadduringrendering.\nNeRF[9,28] 23.85 0.605 0.451\nmip-NeRF[1] 24.04 0.616 0.441\n7.Conclusion\nNeRF++[56] 25.11 0.676 0.375\nPlenoxels[11] 23.08 0.626 0.463\nInstantNGP [32,52] 25.68 0.705 0.302 WepresentedMip-Splatting,amodificationto3DGaussian\nmip-NeRF360[2,30] 27.57 0.793 0.234 Splatting, whichintroducestwonovelfilters, namelya3D\nZip-NeRF[3] 28.54 0.828 0.189 smoothing filter and a 2D Mip filter, to achieve alias-free\n3DGS[18] 27.21 0.815 0.214\nrenderingatarbitraryscales. Our3Dsmoothingfiltereffec-\n3DGS[18]* 27.70 0.826 0.202\n3DGS[18]+EWA[59] 27.77 0.826 0.206 tivelylimitsthemaximalfrequencyofGaussianprimitives\nMip-Splatting(ours) 27.79 0.827 0.203 to match the sampling constraints imposed by the training\nTable4. Single-scaleTrainingandSame-scaleTestingonthe images,whilethe2DMipfilterapproximatestheboxfilter\nMip-NeRF 360 dataset [2]. In the standard in-distribution set- to simulate the physical imaging process. Our experimen-\nting, our approach demonstrates performance on par with many tal results demonstrate that Mip-Splatting is competitive\nestablishedtechniques.∗indicatesthatweretrainthemodel.\nwithstate-of-the-artmethodsintermsofperformancewhen\ntrainingandtestingatthesamescale/samplingrate.Impor-\nsics, is an avenue for future work. As mentioned before, tantly, it significantly outperforms state-of-the-art methods\nthesamplingratecomputationistheonlyprerequisitedur- in out-of-distribution scenarios, when testing at sampling\ning training and the 3D smoothing filter can be fused with ratesdifferentfromtraining, resultinginbettergeneraliza-\nthe Gaussian primitives per Eq. 9, thereby eliminating any tiontoout-of-distributioncameraposesandzoomfactors.\n8\n\nReferences [17] WenboHu,YulingWang,LinMa,BangbangYang,LinGao,\nXiaoLiu,andYuewenMa.Tri-miprf:Tri-miprepresentation\n[1] JonathanT.Barron,BenMildenhall,MatthewTancik,Peter\nforefficientanti-aliasingneuralradiancefields. InProc.of\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nthe IEEE International Conf. on Computer Vision (ICCV),\nMip-nerf: Amultiscalerepresentationforanti-aliasingneu-\n2023. 2,3,6,7,1,4,5,8\nralradiancefields. ICCV,2021. 2,3,6,7,8,1,4,5\n[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¨hler,\n[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nand George Drettakis. 3d gaussian splatting for real-time\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nradiancefieldrendering.ACMTransactionsonGraphics,42\nanti-aliasedneuralradiancefields. CVPR,2022. 2,4,6,7,\n(4),2023. 1,2,3,4,5,6,7,8,9\n8,1,3,9\n[19] JonasKulhanekandTorstenSattler. Tetra-nerf: Represent-\n[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\ning neural radiance fields using tetrahedra. arXiv preprint\nSrinivasan,andPeterHedman. Zip-nerf: Anti-aliasedgrid-\narXiv:2304.09987,2023. 2\nbasedneuralradiancefields.Proc.oftheIEEEInternational\n[20] Christoph Lassner and Michael Zollhofer. Pulsar: Effi-\nConf.onComputerVision(ICCV),2023. 2,3,6,8,7,9\ncient sphere-based neural rendering. In Proceedings of\n[4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and theIEEE/CVFConferenceonComputerVisionandPattern\nHaoSu. Tensorf:Tensorialradiancefields. 2022. 1,2,6,7, Recognition,pages1440–1449,2021. 3\n4,5\n[21] Marc Levoy. Efficient ray tracing of volume data. ACM\n[5] Zhiqin Chen and Hao Zhang. Learning implicit fields for TransactionsonGraphics(TOG),9(3):245–261,1990. 2\ngenerativeshapemodeling. 2019. 2\n[22] Marc Levoy and Pat Hanrahan. Light field rendering. In\n[6] ZhangChen,ZhongLi,LiangchenSong,LeleChen,Jingyi SeminalGraphicsPapers: PushingtheBoundaries,Volume\nYu,JunsongYuan,andYiXu.Neurbf:Aneuralfieldsrepre- 2,pages441–452.2023. 2\nsentation with adaptive radial basis functions. In Proceed- [23] Ruilong Li, Hang Gao, Matthew Tancik, and Angjoo\nings of the IEEE/CVF International Conference on Com-\nKanazawa. Nerfacc:Efficientsamplingacceleratesnerfs. In\nputerVision,pages4182–4194,2023. 2 ProceedingsoftheIEEE/CVFInternationalConferenceon\n[7] RobertL.Cook. Stochasticsamplingincomputergraphics. ComputerVision(ICCV),pages18537–18546,2023. 6\nACMTrans.Graph.,5(1):51–72,1986. 3 [24] LingjieLiu,JiataoGu,KyawZawLin,Tat-SengChua,and\n[8] FranklinC.Crow. Summed-areatablesfortexturemapping. Christian Theobalt. Neural sparse voxel fields. NeurIPS,\nInProceedingsofthe11thAnnualConferenceonComputer 2020. 1,2\nGraphics and Interactive Techniques, page 207–212, New [25] Nelson Max. Optical models for direct volume rendering.\nYork,NY,USA,1984.AssociationforComputingMachin- IEEETransactionsonVisualizationandComputerGraphics,\nery. 3 1(2):99–108,1995. 2\n[9] BoyangDeng,JonathanT.Barron,andPratulP.Srinivasan. [26] NelsonMaxandMinChen.Localandglobalilluminationin\nJaxNeRF:anefficientJAXimplementationofNeRF,2020. the volume rendering integral. Technical report, Lawrence\n8,6 Livermore National Lab.(LLNL), Livermore, CA (United\n[10] RobertADrebin,LorenCarpenter,andPatHanrahan. Vol- States),2005. 2\numerendering. ACMSiggraphComputerGraphics, 22(4): [27] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\n65–74,1988. 2 bastianNowozin,andAndreasGeiger.Occupancynetworks:\n[11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Learning3dreconstructioninfunctionspace. 2019. 2\nChen, BenjaminRecht, andAngjooKanazawa. Plenoxels: [28] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nRadiancefieldswithoutneuralnetworks. InCVPR,2022. 1, JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:\n2,6,8,4 Representingscenesasneuralradiancefieldsforviewsyn-\n[12] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and thesis. InECCV,2020. 1,2,6,7,8,3,4,5\nMichaelFCohen. Thelumigraph. InSeminalGraphicsPa- [29] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla,\npers: Pushing the Boundaries, Volume 2, pages 453–464. Pratul P. Srinivasan, and Jonathan T. Barron. NeRF in the\n2023. 2 dark: High dynamic range view synthesis from noisy raw\n[13] MarkusGrossandHanspeterPfister. Point-basedgraphics. images. CVPR,2022. 2,5\nElsevier,2011. 3 [30] Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter\n[14] JeffreyPGrossmanandWilliamJDally. Pointsampleren- Hedman, Ricardo Martin-Brualla, and Jonathan T. Barron.\ndering. In Rendering Techniques’ 98: Proceedings of the MultiNeRF:ACodeReleaseforMip-NeRF360,Ref-NeRF,\nEurographicsWorkshopinVienna,Austria,June29—July1, andRawNeRF,2022. 8,6\n19989,pages181–192.Springer,1998. 3 [31] KlausMueller,TorstenMoller,JEdwardSwan,RogerCraw-\n[15] PaulSHeckbert. Fundamentalsoftexturemappingandim- fis, Naeem Shareef, and Roni Yagel. Splatting errors and\nagewarping. 1989. 3,5 antialiasing. IEEETransactionsonVisualizationandCom-\n[16] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, puterGraphics,4(2):178–191,1998. 3\nJonathan T Barron, and Paul Debevec. Baking neural ra- [32] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexan-\ndiance fields for real-time view synthesis. In Proceedings derKeller.Instantneuralgraphicsprimitiveswithamultires-\noftheIEEE/CVFInternationalConferenceonComputerVi- olution hash encoding. ACM Trans. Graph., 41(4):102:1–\nsion,pages5875–5884,2021. 2 102:15,2022. 1,2,6,7,8,4,5\n9\n\n[33] HarryNyquist. Certaintopicsintelegraphtransmissionthe- [48] RichardSzeliski. Computervision:algorithmsandapplica-\nory. TransactionsoftheAmericanInstituteofElectricalEn- tions. SpringerNature,2022. 2,5\ngineers,1928. 2,3 [49] OliviaWiles,GeorgiaGkioxari,RichardSzeliski,andJustin\n[34] JeongJoonPark, PeterFlorence, JulianStraub, RichardA. Johnson. SynSin: End-to-end view synthesis from a sin-\nNewcombe,andStevenLovegrove. Deepsdf:Learningcon- gleimage. InProceedingsoftheIEEE/CVFConferenceon\ntinuous signed distance functions for shape representation. ComputerVisionandPatternRecognition(CVPR),2020. 3\n2019. 2 [50] Lance Williams. Pyramidal parametrics. page 1–11, New\n[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, York,NY,USA,1983.AssociationforComputingMachin-\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zem- ery. 3\ning Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai- [51] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nson, Andreas Kopf, Edward Yang, Zachary DeVito, Mar- Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\ntin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Point-based neural radiance fields. In Proceedings of the\nSteiner,LuFang,JunjieBai,andSoumithChintala.Pytorch: IEEE/CVF Conference on Computer Vision and Pattern\nAnimperativestyle,high-performancedeeplearninglibrary. Recognition,pages5438–5448,2022. 2\nInAdvancesinNeuralInformationProcessingSystems32,\n[52] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,\npages8024–8035.CurranAssociates,Inc.,2019. 7\nPratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron,\n[36] Songyou Peng, Chiyu ”Max” Jiang, Yiyi Liao, Michael\nandBenMildenhall.Bakedsdf:Meshingneuralsdfsforreal-\nNiemeyer, Marc Pollefeys, and Andreas Geiger. Shape as timeviewsynthesis. arXiv,2023. 2,8,6\npoints:Adifferentiablepoissonsolver. InAdvancesinNeu- [53] WangYifan,FeliceSerena,ShihaoWu,CengizO¨ztireli,and\nralInformationProcessingSystems(NeurIPS),2021. 3\nOlgaSorkine-Hornung. Differentiablesurfacesplattingfor\n[37] SteveHollaschPeterShirley,TrevorDavidBlack. Raytrac-\npoint-based geometry processing. ACM Transactions on\ninginoneweekend,2023. 2,5\nGraphics (proceedings of ACM SIGGRAPH ASIA), 38(6),\n[38] Hanspeter Pfister, Matthias Zwicker, Jeroen Van Baar, and\n2019. 3\nMarkusGross.Surfels:Surfaceelementsasrenderingprimi-\n[54] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\ntives.InProceedingsofthe27thannualconferenceonCom-\nandAngjooKanazawa. Plenoctreesforreal-timerendering\nputer graphics and interactive techniques, pages 335–342,\nofneuralradiancefields. InProceedingsoftheIEEE/CVF\n2000. 3\nInternationalConferenceonComputerVision,pages5752–\n[39] Sergey Prokudin, Qianli Ma, Maxime Raafat, Julien\n5761,2021. 2\nValentin,andSiyuTang. Dynamicpointfields. InProceed-\n[55] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-\nings of the IEEE/CVF International Conference on Com-\ntler, and Andreas Geiger. Monosdf: Exploring monocu-\nputerVision(ICCV),pages7964–7976,2023. 3\nlar geometric cues for neural implicit surface reconstruc-\n[40] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\ntion. Advances in Neural Information Processing Systems\nGeiger. Kilonerf: Speeding up neural radiance fields with\n(NeurIPS),2022. 4\nthousandsoftinymlps. 2021. 2\n[56] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\n[41] Christian Reiser, Richard Szeliski, Dor Verbin, Pratul P.\nKoltun. Nerf++: Analyzingandimprovingneuralradiance\nSrinivasan, Ben Mildenhall, Andreas Geiger, Jonathan T.\nfields. arXiv:2010.07492,2020. 4,8,6\nBarron, and Peter Hedman. Merf: Memory-efficient radi-\n[57] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J.\nancefieldsforreal-timeviewsynthesisinunboundedscenes.\nBlack,andOtmarHilliges. Pointavatar: Deformablepoint-\nSIGGRAPH,2023. 2\nbased head avatars from videos. In Proceedings of the\n[42] Liu Ren, Hanspeter Pfister, and Matthias Zwicker. Object\nIEEE/CVF Conference on Computer Vision and Pattern\nspace ewa surface splatting: A hardware accelerated ap-\nRecognition(CVPR),2023. 3\nproachtohighqualitypointrendering. InComputerGraph-\n[58] YiyuZhuang,QiZhang,YingFeng,HaoZhu,YaoYao,Xi-\nicsForum,pages461–470.WileyOnlineLibrary,2002. 4\naoyuLi,Yan-PeiCao,YingShan,andXunCao.Anti-aliased\n[43] DariusRu¨ckert,LinusFranke,andMarcStamminger.Adop:\nneuralimplicitsurfaceswithencodinglevelofdetail. arXiv\nApproximatedifferentiableone-pixelpointrendering. arXiv\npreprintarXiv:2309.10336,2023. 3\npreprintarXiv:2110.06635,2021. 3\n[59] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and\n[44] Miguel Sainz and Renato Pajarola. Point-based rendering\nMarkusGross. Ewavolumesplatting. InProceedingsVisu-\ntechniques.Computers&Graphics,28(6):869–879,2004.3\nalization,2001.VIS’01.,pages29–538.IEEE,2001. 2,3,4,\n[45] ClaudeEShannon.Communicationinthepresenceofnoise.\n5,6,7,8,1,9\nProceedingsoftheIRE,1949. 2,3\n[60] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and\n[46] ChengSun,MinSun,andHwann-TzongChen.Directvoxel\nMarkus Gross. Surface splatting. In Proceedings of the\ngridoptimization:Super-fastconvergenceforradiancefields\n28thannualconferenceonComputergraphicsandinterac-\nreconstruction.InProc.IEEEConf.onComputerVisionand\nPatternRecognition(CVPR),2022. 1,2 tivetechniques,pages371–378,2001. 3\n[47] JEdwardSwan,KlausMueller,TorstenMoller,NShareel,\nRogerCrawfis,andRoniYagel. Ananti-aliasingtechnique\nfor splatting. In Proceedings. Visualization’97 (Cat. No.\n97CB36155),pages197–204.IEEE,1997. 3\n10\n\nMip-Splatting: Alias-free 3D Gaussian Splatting\nSupplementary Material\nInthissupplementarydocument,wefirstpresentabla- dataset[2]. Herewepresentanadditionexperimentevalu-\ntionstudiesofMip-SplattinginSection8. Next,wereport atingbothzoom-outandzoom-ineffectsontheMip-NeRF\nadditionalquantitativeandqualityresultsinSection9. 360 dataset [2]. We use the images downsampled by a\nfactor of 4 for training and evaluate it at multiple resolu-\n8.Ablation tions (1/4×, 1/2×, 1×, 2×, 4×). The quantitative results\nare presented in Table 7 and the qualitative comparison is\nIn this section, we evaluate the effectiveness of our 3D\nshowninFigure7. Mip-Splattingsignificantlyoutperforms\nsmoothingfilterand2DMipfilterinSection8.1and Sec-\n3DGS [18] and 3DGS + EWA [59] in rendering quality\ntion8.2. Then,wepresentanadditionalexperimenttoeval-\nwhen zooming in and out, which is consistent to our main\nuatebothzoom-inandzoom-outeffectsinthesamedataset\nresults. Further,removingour3Dsmoothingfilterleadsto\ninSection8.3.\nhigh-frequencyartifacts,whileremovingour2DMip-filter\n8.1.Effectivenessofthe3DSmoothingFilter resultsinaliasingartifacts,asevidencedinFigure7.\nToevaluatetheeffectivenessofthe3Dsmoothingfilter,we 9.AdditionalResults\nconductanablationwiththesingle-scaletrainingandmulti-\nscaletestingsettingtosimulatezoom-ineffectsintheMip- Inthissection,weprovidemorequalitativeandquantitative\nNeRF 360 dataset [2]. The quantitative result is presented results on the Blender dataset [28] in Section 9.1 and the\nin Table5.Omittingthe3Dsmoothingfilterresultsinhigh- Mip-NeRF360dataset[2]inSection9.2.\nfrequencyartifactswhenrenderinghigherresolutionimage,\n9.1.BlenderDataset\nasdepictedinFigure6. Excludingthe2DMipfiltercauses\naslightdeclineinperformanceasthisfilter’sroleismainly We evaluate Mip-Splatting under two different settings in\nfor mitigating zoom-out artifacts, as we will shown next. theBlenderdataset[28].Formulti-scaletrainingandmulti-\nThe absence of both the 3D smoothing filter and the 2D scale testing, the quantitative results are compiled in Ta-\nMipfilterleadstoanexcessivegenerationofsmallGaussian ble 8, where Mip-Splatting achieves state-of-the-art per-\nprimitives,duetothedensitycontrolmechanism,resulting formance. Additionally, per-scene metrics for single-scale\nin out of memory error even on an A100 GPU with 40GB trainingandmulti-scaletestingarepresentedinTable9. A\nmemory. Hence,wedon’treporttheresult. qualitativecomparisonagainstleadingmethodsisshownin\nFigure 8. Mip-Splatting outperforms both 3DGS [18] and\n8.2.Effectivenessofthe2DMipFilter\n3DGS + EWA [59], particularly noticeable when zooming\nout,i.e.atlowerresolutions.\nTo evaluate the effectiveness of the 2D Mip filter, we per-\nform an ablation study with the single-scale training and\n9.2.Mip-NeRF360Dataset\nmulti-scale testing setting to simulate zoom-out effects in\ntheBlenderdataset[28]. Thequantitativeresultsareshown We further evaluate Mip-Splatting on the Mip-NeRF 360\nin Table 6. Upon removing the dilation operation from dataset[2]acrosstwoexperimentalsetups.Inthefirstsetup,\n3DGS[18](3DGS-Dilation),thedilationeffectsareelim- wefollowthestandardapproachwheremodelsaretrained\ninated, outperforming 3DGS in this context. However, it andevaluatedatthesamescale, withindoorscenesdown-\nalsoresultsinaliasingartifactsduetoalackofanti-aliasing. sampled by a factor of two and outdoor scenes by four.\nMip-Splatting outperforms all baseline methods by a large Quantitativeresultswithper-scenemetricsareshowninTa-\nmargin. Removing the 2D Mip filter results in a notable ble 10, our method performs on par with 3DGS [18] and\ndecline in performance, validating its critical role in anti- 3DGS+EWA[59]inthischallengingbenchmark,without\naliasing. Without the 3D smoothing filter, it still produces anydecreaseinperformance.\nalias-free rendering as the 3D filter aims at addressing the In the second setup, models are trained on data down-\nhigh-frequencyartifactswhenzoomingin. sampledbyafactorof8andrenderedatsuccessivelyhigher\nresolutions(1×, 2×, 4×, and8×)tosimulatezoom-inef-\n8.3.Single-scaleTrainingandMulti-scaleTesting\nfects. The quantitative results with per-scene metrics can\nbefoundinTable11. Qualitativecomparisonwithstate-of-\nInthemainpaper,weevaluatethezoom-outeffectsbyren-\nthe-artmethodsareprovidedinFigure9. Mip-Splattingef-\nderinglowerresolutionimagesontheBlenderdataset[28]\nfectively eliminates high-frequency artifacts, yielding high\nfollowing [1, 17] and simulating the zoom-in effects by\nqualityrenderingsthatmorecloselyresemblegroundtruth.\nrendering higher resolution images on the Mip-NeRF 360\n1\n\n3DGS[18] 3DGS[18]+EWA[59] Oursw/o3Dsmoothingfilter Oursw/o2DMipfilter Mip-Splatting(ours) GT\nFigure6. Single-scaleTrainingandMulti-scaleTestingontheMip-NeRF360Dataset[2]. Allmodelsaretrainedonimagesdown-\nsampledbyafactorof8andrenderedatfullresolutiontodemonstratezoom-in/movingclosereffects. Removingthe3Dsmoothingfilter\nresultsinhigh-frequencyartifacts.Mip-Splattingrendersimagesthatcloselyapproximategroundtruth.Zoominforabetterview.\nPSNR↑ SSIM↑ LPIPS↓\n1×Res.2×Res.4×Res.8×Res. Avg. 1×Res.2×Res.4×Res.8×Res. Avg. 1×Res.2×Res.4×Res.8×Res. Avg.\n3DGS[18] 29.19 23.50 20.71 19.59 23.25 0.880 0.740 0.619 0.619 0.715 0.107 0.243 0.394 0.476 0.305\n3DGS[18]+EWA[59] 29.30 25.90 23.70 22.81 25.43 0.880 0.775 0.667 0.643 0.741 0.114 0.236 0.369 0.449 0.292\nMip-Splatting(ours) 29.39 27.39 26.47 26.22 27.37 0.884 0.808 0.754 0.765 0.803 0.108 0.205 0.305 0.392 0.252\nMip-Splatting(ours)w/o3Dsmoothingfilter 29.41 27.09 25.83 25.38 26.93 0.881 0.795 0.722 0.713 0.778 0.107 0.214 0.342 0.424 0.272\nMip-Splatting(ours)w/o2DMipfilter 29.29 27.22 26.31 26.08 27.23 0.882 0.798 0.742 0.759 0.795 0.107 0.214 0.319 0.407 0.262\nTable5.Single-scaleTrainingandMulti-scaleTestingontheMip-NeRF360Dataset[2].Allmethodsaretrainedonthesmallestscale\n(1×)andevaluatedacrossfourscales(1×,2×,4×,and8×),withevaluationsathighersamplingratessimulatingzoom-ineffects.While\nourmethodyieldscomparableresultsatthetrainingresolution,itsignificantlysurpassesallpreviousworkatallotherscales.Omittingthe\n3Dsmoothingfilterresultsinhigh-frequencyartifactswhenrenderinghigherresolutionimageasshownin6,whiletheexcludingthe2D\nMipfilteronlycausesaslightdeclineinperformanceasthisfilter’sroleismainlyformitigatingzoom-outartifacts.\n2\n\nPSNR↑ SSIM↑ LPIPS↓\nFullRes.1/2Res.1/4Res.1/8Res. Avg. FullRes.1/2Res.1/4Res.1/8Res. Avg. FullRes.1/2Res.1/4Res.1/8Res Avg.\n3DGS[18] 33.33 26.95 21.38 17.69 24.84 0.969 0.949 0.875 0.766 0.890 0.030 0.032 0.066 0.121 0.063\n3DGS[18]+EWA[59] 33.51 31.66 27.82 24.63 29.40 0.969 0.971 0.959 0.940 0.960 0.032 0.024 0.033 0.047 0.034\n3DGS[18]-Dilation 33.38 33.06 29.68 26.19 30.58 0.969 0.973 0.964 0.945 0.963 0.030 0.024 0.041 0.075 0.042\nMip-Splatting(ours) 33.36 34.00 31.85 28.67 31.97 0.969 0.977 0.978 0.973 0.974 0.031 0.019 0.019 0.026 0.024\nMip-Splatting(ours)w/o3Dsmoothingfilter 33.67 34.16 31.56 28.20 31.90 0.970 0.977 0.978 0.971 0.974 0.030 0.018 0.019 0.027 0.024\nMip-Splatting(ours)w/o2DMipfilter 33.51 33.38 29.87 26.28 30.76 0.970 0.975 0.966 0.946 0.964 0.031 0.022 0.039 0.073 0.041\nTable6. Single-scaleTrainingandMulti-scaleTestingontheBlenderDataset[28]. Allmethodsaretrainedonfull-resolutionimages\nand evaluated at four different (smaller) resolutions, with lower resolutions simulating zoom-out effects. While Mip-Splatting yields\ncomparableresultsattrainingresolution,itsignificantlysurpassespreviousworkatallotherscales.Removingthe2DMipfilterresultsina\nnotabledeclineinperformanceatlowerresolutions,validatingitscriticalroleinanti-aliasing.Removingthe3Dsmoothingfilterachieves\nsimilarperformancesincethe3Dfilteraimsataddressingthehigh-frequencyartifactswhenzoomingin.\n×4\n×2\n×1\n×2/1×4/1\n×4\n×2\n×1\n×2/1×4/1\n×4\n×2\n×1\n×2/1×4/1\n×4\n×2\n×1\n×2/1×4/1\n3DGS[18] 3DGS[18]+EWA[59] Oursw/o3Dsmoothingfilter Oursw/o2DMipfilter Mip-Splatting(ours) GT\nFigure7.Single-scaleTrainingandMulti-scaleTestingontheMip-NeRF360Dataset[2].Allmethodsaretrainedat1×resolutionand\nevaluatedatdifferentresolutionstomimiczoom-out(1/4×and1/2×)andzoom-in(2×and4×).Mip-Splattingsurpassesboth3DGS[18]\nand3DGS+EWA[59]acrossdifferentresolutions.Removing3Dsmoothingfilterleadstohigh-frquencyartifactswhenzoomingin,while\nomitting2DMipfilterresultsinaliasingartifactswhenzoomingout.\n3\n\nPSNR↑ SSIM↑ LPIPS↓\n1/4Res.1/2Res.1×Res.2×Res.4×Res. Avg. 1/4Res.1/2Res.1×Res.2×Res.4×Res. Avg. 1/4Res.1/2Res.1×Res.2×Res.4×Res. Avg.\n3DGS[18] 20.85 24.66 28.01 25.08 23.37 24.39 0.681 0.812 0.834 0.766 0.735 0.765 0.203 0.158 0.166 0.275 0.383 0.237\n3DGS[18]+EWA[59] 27.40 28.39 28.09 26.43 25.30 27.12 0.888 0.871 0.833 0.774 0.738 0.821 0.103 0.126 0.171 0.276 0.385 0.212\nMip-Splatting(ours) 28.98 29.02 28.09 27.25 26.95 28.06 0.908 0.880 0.835 0.798 0.800 0.844 0.086 0.114 0.168 0.248 0.331 0.189\nMip-Splatting(ours)w/o3Dsmoothingfilter 28.69 28.94 28.05 27.06 26.61 27.87 0.905 0.879 0.833 0.790 0.780 0.837 0.088 0.115 0.168 0.261 0.359 0.198\nMip-Splatting(ours)w/o2DMipfilter 26.09 28.04 28.05 27.27 27.00 27.29 0.815 0.856 0.834 0.798 0.802 0.821 0.167 0.132 0.167 0.249 0.335 0.210\nTable 7. Single-scale Training and Multi-scale Testing on the Mip-NeRF 360 Dataset [2]. All methods are trained on the middle\nscale(1×)andevaluatedacrossfourscales(1/4×,1/2×,1×,2×,and4×),withevaluationsathighersamplingratessimulatingzoom-in\neffects.Whileourmethodyieldscomparableresultsatthetrainingresolution,itsignificantlysurpassesallpreviousworkatallotherscales.\nOmittingthe3Dsmoothingfilterresultsinhigh-frequencyartifactswhenrenderinghigherresolutionimages,whileremovingthe2DMip\nfilterresultsinaliasingartifactswhenrenderinglowerresolutionimages,asshownin Figure7.\nPSNR\nchair drums ficus hotdog lego materials mic ship Average\nNeRFw/oL [1,28] 29.92 23.27 27.15 32.00 27.75 26.30 28.40 26.46 27.66\narea\nNeRF[28] 33.39 25.87 30.37 35.64 31.65 30.18 32.60 30.09 31.23\nMipNeRF[1] 37.14 27.02 33.19 39.31 35.74 32.56 38.04 33.08 34.51\nPlenoxels[11] 32.79 25.25 30.28 34.65 31.26 28.33 31.53 28.59 30.34\nTensoRF[4] 32.47 25.37 31.16 34.96 31.73 28.53 31.48 29.08 30.60\nInstant-ngp[32] 32.95 26.43 30.41 35.87 31.83 29.31 32.58 30.23 31.20\nTri-MipRF[17]* 37.67 27.35 33.57 38.78 35.72 31.42 37.63 32.74 34.36\n3DGS[18] 32.73 25.30 29.00 35.03 29.44 27.13 31.17 28.33 29.77\n3DGS[18]+EWA[59] 35.77 27.14 33.65 37.74 32.75 30.21 35.21 31.63 33.01\nMip-Splatting(ours) 37.48 27.74 34.71 39.15 35.07 31.88 37.68 32.80 34.56\nSSIM\nchair drums ficus hotdog lego materials mic ship Average\nNeRFw/oL [1,28] 0.944 0.891 0.942 0.959 0.926 0.934 0.958 0.861 0.927\narea\nNeRF[28] 0.971 0.932 0.971 0.979 0.965 0.967 0.980 0.900 0.958\nMipNeRF[1] 0.988 0.945 0.984 0.988 0.984 0.977 0.993 0.922 0.973\nPlenoxels[11] 0.968 0.929 0.972 0.976 0.964 0.959 0.979 0.892 0.955\nTensoRF[4] 0.967 0.930 0.974 0.977 0.967 0.957 0.978 0.895 0.956\nInstant-ngp[32] 0.971 0.940 0.973 0.979 0.966 0.959 0.981 0.904 0.959\nTri-MipRF[17]* 0.990 0.951 0.985 0.988 0.986 0.969 0.992 0.929 0.974\n3DGS[18] 0.976 0.941 0.968 0.982 0.964 0.956 0.979 0.910 0.960\n3DGS[18]+EWA[59] 0.986 0.958 0.988 0.988 0.979 0.972 0.990 0.929 0.974\nMip-Splatting(ours) 0.991 0.963 0.990 0.990 0.987 0.978 0.994 0.936 0.979\nLPIPS\nchair drums ficus hotdog lego materials mic ship Average\nNeRFw/oL [1,28] 0.035 0.069 0.032 0.028 0.041 0.045 0.031 0.095 0.052\narea\nNeRF[28] 0.028 0.059 0.026 0.024 0.035 0.033 0.025 0.085 0.044\nMipNeRF[1] 0.011 0.044 0.014 0.012 0.013 0.019 0.007 0.062 0.026\nPlenoxels[11] 0.040 0.070 0.032 0.037 0.038 0.055 0.036 0.104 0.051\nTensoRF[4] 0.042 0.075 0.032 0.035 0.036 0.063 0.040 0.112 0.054\nInstant-ngp[32] 0.035 0.066 0.029 0.028 0.040 0.051 0.032 0.095 0.047\nTri-MipRF[17]* 0.011 0.046 0.016 0.014 0.013 0.033 0.008 0.069 0.026\n3DGS[18] 0.025 0.056 0.030 0.022 0.038 0.040 0.023 0.086 0.040\n3DGS[18]+EWA[59] 0.017 0.039 0.013 0.016 0.024 0.026 0.011 0.070 0.027\nMip-Splatting(ours) 0.010 0.031 0.009 0.011 0.012 0.018 0.005 0.059 0.019\nTable8. Multi-scaleTrainingandMulti-scaleTestingonthetheBlenderdataset[28]. Foreachscene,wereportthearithmeticmean\nofeachmetricaveragedoverthe4scalesusedinthedataset.\n4\n\nPSNR\nchair drums ficus hotdog lego materials mic ship Average\nNeRF[28] 31.99 25.31 30.74 34.45 30.69 28.86 31.41 28.36 30.23\nMipNeRF[1] 32.89 25.58 31.80 35.40 32.24 29.46 33.26 29.88 31.31\nTensoRF[4] 32.17 25.51 31.19 34.69 31.46 28.60 31.50 28.71 30.48\nInstant-ngp[32] 32.18 25.05 31.32 34.85 31.53 28.59 32.15 28.84 30.57\nTri-MipRF[17] 32.48 24.01 28.41 34.45 30.41 27.82 31.19 27.02 29.47\n3DGS[18] 26.81 21.17 26.02 28.80 25.36 23.10 24.39 23.05 24.84\n3DGS[18]+EWA[59] 32.85 24.91 31.94 33.33 29.76 27.36 27.68 27.41 29.40\nMip-Splatting(ours) 35.69 26.50 32.99 36.18 32.76 30.01 31.66 29.98 31.97\nSSIM\nchair drums ficus hotdog lego materials mic ship Average\nNeRF[28] 0.968 0.936 0.976 0.977 0.963 0.964 0.980 0.887 0.956\nMipNeRF[1] 0.974 0.939 0.981 0.982 0.973 0.969 0.987 0.915 0.965\nTensoRF[4] 0.970 0.938 0.978 0.979 0.970 0.963 0.981 0.906 0.961\nInstant-ngp[32] 0.970 0.935 0.977 0.980 0.969 0.962 0.982 0.909 0.961\nTri-MipRF[17] 0.971 0.908 0.957 0.975 0.957 0.953 0.975 0.883 0.947\n3DGS[18] 0.915 0.851 0.921 0.930 0.882 0.882 0.909 0.827 0.890\n3DGS[18]+EWA[59] 0.978 0.942 0.983 0.977 0.964 0.958 0.963 0.912 0.960\nMip-Splatting(ours) 0.988 0.958 0.988 0.987 0.982 0.974 0.986 0.930 0.974\nLPIPS\nchair drums ficus hotdog lego materials mic ship Average\nNeRF[28] 0.040 0.067 0.027 0.034 0.043 0.049 0.035 0.132 0.053\nMipNeRF[1] 0.033 0.062 0.022 0.025 0.030 0.041 0.023 0.092 0.041\nTensoRF[4] 0.036 0.066 0.027 0.030 0.035 0.052 0.034 0.102 0.048\nInstant-ngp[32] 0.036 0.074 0.035 0.030 0.035 0.054 0.034 0.096 0.049\nTri-MipRF[17] 0.026 0.086 0.041 0.023 0.036 0.048 0.023 0.117 0.050\n3DGS[18] 0.047 0.087 0.055 0.034 0.064 0.055 0.046 0.113 0.063\n3DGS[18]+EWA[59] 0.023 0.051 0.017 0.018 0.033 0.027 0.024 0.077 0.034\nMip-Splatting(ours) 0.014 0.035 0.012 0.014 0.016 0.019 0.015 0.066 0.024\nTable9. Single-scaleTrainingandMulti-scaleTestingonthetheBlenderdataset[28]. Foreachscene,wereportthearithmeticmean\nofeachmetricaveragedoverthefourscalesusedinthedataset.\n5\n\nPSNR\nbicycle flowers garden stump treehill room counter kitchen bonsai\nNeRF[9,28] 21.76 19.40 23.11 21.73 21.28 28.56 25.67 26.31 26.81\nmip-NeRF[1] 21.69 19.31 23.16 23.10 21.21 28.73 25.59 26.47 27.13\nNeRF++[56] 22.64 20.31 24.32 24.34 22.20 28.87 26.38 27.80 29.15\nPlenoxels[11] 21.91 20.10 23.49 20.661 22.25 27.59 23.62 23.42 24.67\nInstantNGP [32,52] 22.79 19.19 25.26 24.80 22.46 30.31 26.21 29.00 31.08\nmip-NeRF360[2,30] 24.40 21.64 26.94 26.36 22.81 31.40 29.44 32.02 33.11\nZip-NeRF[3] 25.80 22.40 28.20 27.55 23.89 32.65 29.38 32.50 34.46\n3DGS[18] 25.25 21.52 27.41 26.55 22.49 30.63 28.70 30.32 31.98\n3DGS[18]* 25.63 21.77 27.70 26.87 22.75 31.69 29.08 31.56 32.29\n3DGS[18]+EWA[59] 25.64 21.86 27.65 26.87 22.91 31.68 29.21 31.59 32.51\nMip-Splatting(ours) 25.72 21.93 27.76 26.94 22.98 31.74 29.16 31.55 32.31\nSSIM\nbicycle flowers garden stump treehill room counter kitchen bonsai\nNeRF[9,28] 0.455 0.376 0.546 0.453 0.459 0.843 0.775 0.749 0.792\nmip-NeRF[1] 0.454 0.373 0.543 0.517 0.466 0.851 0.779 0.745 0.818\nNeRF++[56] 0.526 0.453 0.635 0.594 0.530 0.852 0.802 0.816 0.876\nPlenoxels[11] 0.496 0.431 0.606 0.523 0.509 0.842 0.759 0.648 0.814\nInstantNGP [32,52] 0.540 0.378 0.709 0.654 0.547 0.893 0.845 0.857 0.924\nmip-NeRF360[2,30] 0.693 0.583 0.816 0.746 0.632 0.913 0.895 0.920 0.939\nZip-NeRF[3] 0.769 0.642 0.860 0.800 0.681 0.925 0.902 0.928 0.949\n3DGS[18] 0.771 0.605 0.868 0.775 0.638 0.914 0.905 0.922 0.938\n3DGS[18]* 0.777 0.622 0.873 0.783 0.652 0.928 0.916 0.933 0.948\n3DGS[18]+EWA[59] 0.777 0.620 0.871 0.784 0.655 0.927 0.916 0.933 0.948\nMip-Splatting(ours) 0.780 0.623 0.875 0.786 0.655 0.928 0.916 0.933 0.948\nLPIPS\nbicycle flowers garden stump treehill room counter kitchen bonsai\nNeRF[9,28] 0.536 0.529 0.415 0.551 0.546 0.353 0.394 0.335 0.398\nmip-NeRF[1] 0.541 0.535 0.422 0.490 0.538 0.346 0.390 0.336 0.370\nNeRF++[56] 0.455 0.466 0.331 0.416 0.466 0.335 0.351 0.260 0.291\nPlenoxels[11] 0.506 0.521 0.3864 0.503 0.540 0.419 0.441 0.447 0.398\nInstantNGP [32,52] 0.398 0.441 0.255 0.339 0.420 0.242 0.255 0.170 0.198\nmip-NeRF360[2,30] 0.289 0.345 0.164 0.254 0.338 0.211 0.203 0.126 0.177\nZip-NeRF[3] 0.208 0.273 0.118 0.193 0.242 0.196 0.185 0.116 0.173\n3DGS[18] 0.205 0.336 0.103 0.210 0.317 0.220 0.204 0.129 0.205\n3DGS[18]* 0.205 0.329 0.103 0.208 0.318 0.192 0.178 0.113 0.174\n3DGS[18]+EWA[59] 0.213 0.335 0.111 0.210 0.325 0.192 0.179 0.113 0.173\nMip-Splatting(ours) 0.206 0.331 0.103 0.209 0.320 0.192 0.179 0.113 0.173\nTable10.Single-scaleTrainingandSingle-scaleTestingontheMip-NeRF360dataset[2].Indoorscenesaredownsampledbyafactor\nof2andoutdoorscenesby4.\n6\n\nPSNR\nbicycle flowers garden stump treehill room counter kitchen bonsai\nInstant-NGP[32] 22.51 20.25 24.65 23.15 22.24 29.48 26.18 27.10 29.66\nmip-NeRF360[2] 24.21 21.60 25.82 25.59 22.78 22.95 27.72 28.78 31.63\nzip-NeRF[3] 23.05 20.05 18.07 23.94 22.53 20.51 26.08 27.37 30.05\n3DGS[18] 21.34 19.43 21.94 22.63 20.91 28.10 25.33 23.68 25.89\n3DGS[18]+EWA[59] 23.74 20.94 24.69 24.81 21.93 29.80 27.23 27.07 28.63\nMip-Splatting(ours) 25.26 22.02 26.78 26.65 22.92 31.56 28.87 30.73 31.49\nSSIM\nbicycle flowers garden stump treehill room counter kitchen bonsai\nInstant-NGP[32] 0.538 0.473 0.647 0.590 0.544 0.868 0.795 0.764 0.877\nmip-NeRF360[2] 0.662 0.567 0.716 0.715 0.628 0.795 0.845 0.828 0.910\nzip-NeRF[3] 0.640 0.521 0.548 0.661 0.590 0.655 0.784 0.800 0.865\n3DGS[18] 0.638 0.536 0.675 0.662 0.591 0.878 0.826 0.789 0.838\n3DGS[18]+EWA[59] 0.671 0.563 0.718 0.693 0.608 0.889 0.843 0.813 0.874\nMip-Splatting(ours) 0.738 0.613 0.786 0.776 0.659 0.921 0.897 0.903 0.933\nLPIPS\nbicycle flowers garden stump treehill room counter kitchen bonsai\nInstant-NGP[32] 0.500 0.486 0.372 0.469 0.511 0.270 0.310 0.286 0.229\nmip-NeRF360[2] 0.358 0.400 0.296 0.333 0.391 0.256 0.228 0.210 0.182\nzip-NeRF[3] 0.353 0.397 0.346 0.349 0.366 0.302 0.277 0.232 0.236\n3DGS[18] 0.336 0.406 0.295 0.353 0.406 0.223 0.239 0.245 0.242\n3DGS[18]+EWA[59] 0.322 0.395 0.281 0.334 0.405 0.217 0.231 0.216 0.227\nMip-Splatting(ours) 0.281 0.373 0.233 0.281 0.369 0.193 0.199 0.165 0.176\nTable11. Single-scaleTrainingandMulti-scaleTestingontheMip-NeRF360dataset[2]. Allmodelsaretrainedonimagesdown-\nsampledbyafactorof8andrenderedathigherresolutionstosimulateszoom-ineffects.\n7\n\nlluF\n2/1\n4/1\n8/1\nlluF\n2/1\n4/1\n8/1\nlluF\n2/1\n4/1\n8/1\nlluF\n2/1\n4/1\n8/1\nlluF\n2/1\n4/1\n8/1\nlluF\n2/1\n4/1\n8/1\nlluF\n2/1\n4/1\n8/1\nMip-NeRF[1] Tri-MipRF[17] 3DGS[18] 3DGS[18]+EWA[59] Mip-Splatting(ours) GT\nFigure8. Single-scaleTrainingandMulti-scaleTestingontheBlenderDataset[28]. Allmethodsaretrainedatfullresolutionand\nevaluatedatdifferent(smaller)resolutionstomimiczoom-out.Methodsbasedon3DGScapturefinedetailsbetterthanMip-NeRF[1]and\nTri-MipRF[17]attrainingresolution.Mip-Splattingsurpassesboth3DGS[18]and3DGS+EWA[59]atlowerresolutions.\n8\n\nMip-NeRF360[2] Zip-NeRF[3] 3DGS[18] 3DGS[18]+EWA[59] Mip-Splatting(ours) GT\nFigure9. Single-scaleTrainingandMulti-scaleTestingontheMip-NeRF360Dataset[2]. Allmodelsaretrainedonimagesdown-\nsampled by a factor of eight and rendered at full resolution to demonstrate zoom-in/moving closer effects. In contrast to prior work,\nMip-Splattingrendersimagesthatcloselyapproximategroundtruth.Pleasealsonotethehigh-frequencyartifactsof3DGS+EWA[59].\n9"
}