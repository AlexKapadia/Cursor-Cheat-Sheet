---
alwaysApply: false
---

# Deep Learning Introduction

## Paper Metadata
- **Title:** Deep Learning Introduction (Chapter 4)
- **Source Book:** Artificial Intelligence, Machine Learning, and Deep Learning
- **Author:** Oswald Campesato
- **Year:** 2020
- **Publisher:** Mercury Learning and Information
- **ISBN:** 978-1-68392-467-8
- **Chapter:** 4

## Abstract / Summary

This chapter introduces deep learning, including MLPs (Multilayer Perceptrons) and CNNs (Convolutional Neural Networks). Other deep learning architectures such as RNNs (Recurrent Neural Networks) and LSTMs (Long Short Term Memory) are discussed in Chapter 5.

The chapter covers deep learning fundamentals, problems it can solve, challenges, Perceptrons (core building blocks for neural networks), MLPs, CNNs, and practical code examples using Keras/TensorFlow 2 with the MNIST dataset.

## Problem Statement

### Problem Definition

Deep learning is a subset of machine learning that focuses on neural networks and algorithms for training neural networks. A deep learning model requires at least two hidden layers in a neural network (very deep learning involves neural networks with at least ten hidden layers).

### Motivation

Deep learning can solve problems that traditional machine learning cannot:
- Image classification and recognition
- Natural language processing
- Speech recognition
- Handwriting recognition
- Complex pattern recognition
- Sequential data processing

### Challenges

- Bias in algorithms
- Susceptibility to adversarial attacks
- Limited ability to generalize
- Lack of explainability
- Correlation but not causality
- Vanishing gradient problem
- Exploding gradient problem
- Overfitting

### Scope

This chapter covers:
- Deep learning fundamentals
- Perceptrons and ANNs
- MLPs (Multilayer Perceptrons)
- CNNs (Convolutional Neural Networks)
- Hyperparameters
- Forward and backward propagation
- Code examples with Keras/TensorFlow 2
- MNIST dataset examples

## Key Concepts and Techniques

1. **Deep Learning:** Subset of ML focusing on neural networks with multiple hidden layers
2. **Perceptron:** Core building block for neural networks
3. **ANN (Artificial Neural Network):** Network of interconnected neurons
4. **MLP (Multilayer Perceptron):** Fully connected feed-forward neural network
5. **CNN (Convolutional Neural Network):** Deep NN with convolutional layers for images
6. **Forward Propagation:** Data flow from input to output layer
7. **Backward Propagation (Backprop):** Weight updates from output to input layer
8. **Hyperparameters:** Configuration parameters set before training
9. **Activation Functions:** Nonlinear functions applied to neuron outputs
10. **Loss Function:** Measures difference between predictions and actual values
11. **Optimizer:** Algorithm for updating weights during training
12. **Learning Rate:** Controls step size in weight updates
13. **Dropout:** Regularization technique to prevent overfitting
14. **Convolution:** Filter operation on images to create feature maps
15. **Max Pooling:** Downsampling technique to reduce feature map size

## Algorithms

### Algorithm 1: Perceptron

**Description:**
A Perceptron is a function $f(x)$ where:
$$
f(x) = \begin{cases}
1 & \text{if } w \cdot x + b > 0 \\
0 & \text{otherwise}
\end{cases}
$$

**Components:**
- $w$: vector of weights
- $x$: input vector
- $b$: bias vector
- $w \cdot x$: inner product (dot product)

**Characteristics:**
- All-or-nothing decision (binary output)
- Core building block for neural networks
- Used in ANNs, MLPs, RNNs, LSTMs, VAEs
- Stateless (does not retain information about previously processed data)

**Weighted Sum Calculation:**
For a neuron $N'$ receiving inputs with weights $\{w_1, w_2, w_3, \ldots, w_n\}$:
$$
\text{weighted sum} = x_1 \cdot w_1 + x_2 \cdot w_2 + \ldots + x_n \cdot w_n
$$

### Algorithm 2: Forward Propagation

**Description:**
Forward propagation is the process of passing input data through the neural network from the input layer to the output layer.

**Steps:**
1. Start with input vector $x_1$
2. Multiply by weight matrix $W_1$ connecting input to first hidden layer: result is $x_2$
3. Apply activation function to each element of $x_2$: create $x_3$
4. Repeat steps 2-3 for subsequent layers until output layer

**Process:**
- Data flows left-to-right (input → hidden → output)
- Each layer's output becomes next layer's input
- Activation functions applied at each layer
- Final output contains predictions/estimates

### Algorithm 3: Backward Error Propagation (Backprop)

**Description:**
Backward error propagation calculates numbers used to update weights of edges in the neural network, starting from the output layer and moving right-to-left toward the input layer.

**Steps:**
1. Calculate loss at output layer
2. Compute gradients using chain rule
3. Update weights using optimizer and learning rate
4. Propagate gradients backward through layers
5. Repeat for each data point in training set

**Purpose:**
- Train the neural network
- Reduce loss between estimated and true values
- Update weights to improve accuracy

**Key Components:**
- Loss function (MSE, cross-entropy, etc.)
- Optimizer (Adam, SGD, RMSprop, etc.)
- Learning rate (typically 0.001 to 0.05)

### Algorithm 4: Multilayer Perceptron (MLP)

**Description:**
A multilayer perceptron (MLP) is a feed-forward artificial neural network that consists of at least three layers of nodes: an input layer, a hidden layer, and an output layer.

**Characteristics:**
- Fully connected: every node in left layer connected to every node in right layer
- Each node (except input) is a neuron with nonlinear activation function
- Uses backward error propagation for training
- Can handle nonlinearly separable data (unlike single-layer perceptrons)

**Architecture:**
- Input layer: receives input data
- Hidden layers: one or more layers of neurons
- Output layer: produces final predictions

**Activation Functions:**
- Must use nonlinear activation functions (sigmoid, tanh, ReLU)
- Without activation functions, MLP becomes linear system
- Linear system can be reduced to single matrix (defeats purpose of multiple layers)

**XOR Problem:**
- Single-layer perceptrons cannot learn XOR function
- Two-layer perceptron (MLP) can learn XOR function
- Demonstrates need for multiple layers

### Algorithm 5: Convolutional Neural Network (CNN)

**Description:**
CNNs are deep neural networks with one or more convolutional layers, well-suited for image classification, audio processing, and NLP tasks.

**Architecture Components:**
1. **Conv2D Layer:** Convolutional layer with filters
2. **ReLU Activation:** Applied to feature maps
3. **Max Pooling:** Downsampling technique
4. **Flatten:** Convert 2D arrays to 1D vectors
5. **Fully Connected (FC) Layer:** Dense layer
6. **Softmax:** Output layer activation for classification

**Convolution Operation:**
- Filters (small square matrices: 3×3, 5×5, 7×7, or 1×1)
- Scanned across image
- Inner product calculated at each position
- Result: feature map containing real numbers

**Max Pooling:**
- Partition feature map into 2×2 rectangles
- Select maximum value from each rectangle
- Reduces size to 25% (discards 75% of numbers)
- Alternative: average pooling, L2 pooling

**Translation Invariance:**
- Image recognized regardless of position in bitmap
- Key advantage over MLPs for computer vision

**Advantages over MLPs:**
- Do not require adjacent layers to be fully connected
- More scalable for computer vision tasks
- Translation invariance
- Shared weights (more computationally feasible)

## Code Examples and Snippets

### Code Example 1: XOR Function with Keras

**Context:** Demonstrates neural network learning nonlinearly separable XOR function

**Language:** Python

```python
import tensorflow as tf
import numpy as np

# Logical XOR operator and "truth" values:
x = np.array([[0., 0.],[0., 1.],[1., 0.],[1., 1.]])
y = np.array([[0.], [1.], [1.], [0.]])

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(2, input_dim=2, activation='relu'))
model.add(tf.keras.layers.Dense(1))

print("compiling model...")
model.compile(loss='mean_squared_error', optimizer='adam')
print("fitting model...")
model.fit(x, y, verbose=0, epochs=1000)

pred = model.predict(x)

# Test final prediction
print("Testing XOR operator")
p1 = np.array([[0., 0.]])
p2 = np.array([[0., 1.]])
p3 = np.array([[1., 0.]])
p4 = np.array([[1., 1.]])

print(p1, ":", model.predict(p1))
print(p2, ":", model.predict(p2))
print(p3, ":", model.predict(p3))
print(p4, ":", model.predict(p4))
```

**Output:**
```
compiling model...
fitting model...
Testing XOR operator
[[0. 0.]] : [[0.36438465]]
[[0. 1.]] : [[1.0067574]]
[[1. 0.]] : [[0.36437267]]
[[1. 1.]] : [[0.15084022]]
```

**Other Logic Gates:**
```python
# NOR
y = np.array([[1.], [0.], [0.], [1.]])

# OR
y = np.array([[0.], [1.], [1.], [1.]])

# AND
y = np.array([[0.], [0.], [0.], [1.]])
```

### Code Example 2: MNIST with Keras (MLP)

**Context:** Simple neural network for MNIST digit classification using Flatten layer

**Language:** Python

```python
import tensorflow as tf

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize pixel values to 0-1
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation=tf.nn.relu),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.summary()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
```

**Model Summary:**
```
Model: "sequential"
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
dense (Dense)                (None, 512)                401920    
dropout (Dropout)            (None, 512)                0         
dense_1 (Dense)              (None, 10)                 5130      
=================================================================
Total params: 407,050
Trainable params: 407,050
Non-trainable params: 0
```

**Expected Accuracy:** ~98.6%

### Code Example 3: MNIST with CNN

**Context:** Convolutional neural network for MNIST digit classification

**Language:** Python

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

# Reshape for CNN: (samples, height, width, channels)
train_images = train_images.reshape((60000, 28, 28, 1))
test_images = test_images.reshape((10000, 28, 28, 1))

# Normalize pixel values: from 0-255 to 0-1
train_images, test_images = train_images/255.0, test_images/255.0

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.summary()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=1)
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(test_acc)

# Predict the label of one image
test_image = np.expand_dims(test_images[300], axis=0)
plt.imshow(test_image.reshape(28, 28))
plt.show()
result = model.predict(test_image)
print("result:", result)
print("result.argmax():", result.argmax())
```

**Model Summary:**
```
Model: "sequential"
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 26, 26, 32)        320       
max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         
conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     
max_pooling2d_1 (MaxPooling2D) (None, 5, 5, 64)         0         
conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     
flatten (Flatten)             (None, 576)               0         
dense (Dense)                 (None, 64)                36928     
dense_1 (Dense)               (None, 10)                650       
=================================================================
Total params: 93,322
Trainable params: 93,322
Non-trainable params: 0
```

**Expected Accuracy:** ~98.7%

### Code Example 4: Displaying MNIST Image

**Context:** Visualizing MNIST dataset images

**Language:** Python

```python
import tensorflow as tf
import matplotlib.pyplot as plt

mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print("X_train.shape:", X_train.shape)
print("X_test.shape:", X_test.shape)

first_img = X_train[0]
# uncomment this line to see the pixel values
# print(first_img)

plt.imshow(first_img, cmap='gray')
plt.show()
```

**Output:**
```
X_train.shape: (60000, 28, 28)
X_test.shape: (10000, 28, 28)
```

### Code Example 5: Linear Regression with Deep Learning

**Context:** Simple deep learning approach to linear regression

**Language:** Python

```python
import tensorflow as tf

# Initialize variables
m = tf.Variable(0.)
b = tf.Variable(0.)

# Prediction function
def predict(x):
    y = m * x + b
    return y

# Loss function (MSE)
def squared_error(y_pred, y_actual):
    return tf.reduce_mean(tf.square(y_pred - y_actual))

# Training data
x_train = # your training data
y_train = # your training labels

# Calculate loss
loss = squared_error(predict(x_train), y_train)
print("Loss:", loss.numpy())
```

## Mathematical Foundations

### Notation

- $x$: Input vector
- $W$: Weight matrix
- $b$: Bias vector
- $y$: Output/prediction
- $h$: Hidden layer output
- $f$: Activation function
- $\sigma$: Sigmoid function
- $\alpha$: Learning rate
- $L$: Loss function

### Core Formulas

#### Perceptron Function

$$
f(x) = \begin{cases}
1 & \text{if } w \cdot x + b > 0 \\
0 & \text{otherwise}
\end{cases}
$$

#### Weighted Sum

For neuron with inputs $x_1, x_2, \ldots, x_n$ and weights $w_1, w_2, \ldots, w_n$:
$$
\text{weighted sum} = \sum_{i=1}^{n} x_i \cdot w_i = x_1 w_1 + x_2 w_2 + \ldots + x_n w_n
$$

#### Forward Propagation (RNN-like)

For RNN at time $t$:
$$
h(t) = f(W \cdot x(t) + U \cdot h(t-1))
$$

Where:
- $W$: weight matrix for input
- $U$: weight matrix for previous hidden state
- $f$: activation function (typically tanh)

#### Loss Functions

**Mean Squared Error (MSE):**
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_{\text{pred}}^{(i)} - y_{\text{actual}}^{(i)})^2
$$

**Cross-Entropy Loss:**
$$
\text{Cross-Entropy} = -\sum_{i=1}^{n} y_{\text{actual}}^{(i)} \log(y_{\text{pred}}^{(i)})
$$

#### Weight Update (Gradient Descent)

$$
w_{\text{new}} = w_{\text{old}} - \alpha \cdot \frac{\partial L}{\partial w}
$$

Where:
- $\alpha$: learning rate
- $\frac{\partial L}{\partial w}$: gradient of loss with respect to weight

#### Convolution Operation

For 2D convolution:
$$
(f * g)(i, j) = \sum_{m} \sum_{n} f(m, n) \cdot g(i - m, j - n)
$$

In practice, this is the inner product of filter and image patch.

### Variable Definitions

- **Epoch:** One complete pass through the entire training dataset
- **Batch:** Subset of training data processed in one iteration
- **Batch Size:** Number of samples in one batch
- **Feature Map:** Output of convolution operation
- **Filter/Kernel:** Small matrix used in convolution
- **Stride:** Step size when moving filter across image
- **Padding:** Adding zeros around image border
- **Pooling:** Downsampling operation (max, average, etc.)

## Hyperparameters

### Required for Initialization

1. **Number of Hidden Layers:**
   - Deep learning: at least 2 hidden layers
   - Very deep learning: at least 10 hidden layers

2. **Number of Neurons in Hidden Layers:**
   - Determines model capacity
   - Too few: underfitting
   - Too many: overfitting

3. **Weight Initialization:**
   - Small random numbers (typically 0 to 1)
   - Xavier initialization
   - He initialization

### Required for Forward Propagation

4. **Activation Function:**
   - ReLU (preferred for hidden layers)
   - Sigmoid (for gates in LSTMs)
   - Tanh (for cell state in LSTMs)
   - Softmax (for output layer in classification)

### Required for Backward Propagation

5. **Cost/Loss Function:**
   - MSE (Mean Squared Error)
   - Cross-entropy
   - Most complex hyperparameter

6. **Optimizer:**
   - SGD (Stochastic Gradient Descent)
   - RMSprop
   - Adagrad
   - Adadelta
   - Adam (commonly used)

7. **Learning Rate:**
   - Typically 0.001 to 0.05
   - Too large: overshoot optimal point
   - Too small: slow training

### Optional (Regularization)

8. **Dropout Rate:**
   - Decimal value between 0 and 1 (typically 0.2 to 0.5)
   - Percentage of neurons randomly ignored during forward pass
   - Reduces overfitting
   - Implemented via `tf.keras.layers.Dropout`

## Deep Learning Architectures

### MLP (Multilayer Perceptron)

**Characteristics:**
- Fully connected feed-forward network
- At least 3 layers (input, hidden, output)
- Nonlinear activation functions
- Uses backprop for training

**Use Cases:**
- Classification
- Regression
- Not scalable for computer vision

**Limitations:**
- Not scalable for computer vision tasks
- Somewhat difficult to train
- Fully connected layers can be computationally expensive

### CNN (Convolutional Neural Network)

**Characteristics:**
- One or more convolutional layers
- Shared weights (more efficient)
- Translation invariance
- Not fully connected

**Use Cases:**
- Image classification
- Audio processing
- NLP tasks
- Object detection

**Advantages:**
- Scalable for computer vision
- Translation invariance
- More computationally feasible than MLPs

### RNN (Recurrent Neural Network)

**Characteristics:**
- Stateful (retains information)
- Feedback mechanism
- Suitable for sequential data
- Discussed in Chapter 5

### LSTM (Long Short Term Memory)

**Characteristics:**
- Special type of RNN
- Three gates (forget, input, output)
- Long-term memory cell
- Discussed in Chapter 5

### Combined Architectures

**Video Processing:**
- CNN: process each image in video sequence
- LSTM: make predictions of object positions

**NLP:**
- Combination of CNNs, RNNs, LSTMs
- Bidirectional LSTMs
- Transformers (BERT, etc.)

## Problems Deep Learning Can Solve

### Gradient Problems

**Vanishing Gradient:**
- Gradient becomes very close to zero
- Weights no longer updated
- Neural network becomes inert
- Mitigated by LSTMs and ReLU

**Exploding Gradient:**
- Gradient becomes arbitrarily large
- Can cause training instability
- Mitigated by gradient clipping, truncated BPTT

**Solutions:**
- Use LSTMs instead of simple RNNs
- Replace sigmoid with ReLU
- Gradient clipping
- Truncated BPTT

### Applications

1. **Image Classification:** CNNs for recognizing objects in images
2. **Speech Recognition:** RNNs/LSTMs for audio processing
3. **Natural Language Processing:** RNNs/LSTMs/Transformers
4. **Handwriting Recognition:** RNNs/LSTMs
5. **Video Analysis:** CNN + LSTM combinations
6. **Audio Signal Analysis:** CNNs for sound classification

## Challenges in Deep Learning

### 1. Bias in Algorithms

- Unintentional bias in algorithms
- Bias in training data
- Example: Dataset with Caucasian males/females → model determined males = physicians, females = housewives
- Solution: Careful dataset curation, bias detection

### 2. Susceptibility to Adversarial Attacks

- Small perturbations can fool models
- Security concern for production systems
- Active area of research

### 3. Limited Ability to Generalize

- Models find patterns in datasets
- Generalizing to new data is difficult
- Overfitting is common problem

### 4. Lack of Explainability

- Neural networks are "black boxes"
- Difficult to understand why model made decision
- Explainable AI is active research area

### 5. Correlation but Not Causality

- Deep learning finds patterns and correlations
- Cannot determine causality
- Important limitation for scientific applications

## Best Practices and Recommendations

### Architecture Selection

1. **For Images:** Use CNNs
2. **For Sequences:** Use RNNs/LSTMs
3. **For Simple Problems:** Start with MLPs
4. **For Complex Patterns:** Use deep architectures

### Hyperparameter Tuning

1. **Start Simple:** Begin with small networks
2. **Increase Complexity:** Add layers/neurons gradually
3. **Monitor Overfitting:** Use validation set
4. **Regularization:** Use dropout if overfitting
5. **Learning Rate:** Start with 0.01, adjust as needed

### Training Tips

1. **Data Preprocessing:**
   - Normalize inputs (0-1 or standardized)
   - One-hot encode categorical outputs
   - Split into train/validation/test sets

2. **Initialization:**
   - Use appropriate weight initialization
   - Xavier or He initialization for deep networks

3. **Activation Functions:**
   - ReLU for hidden layers
   - Softmax for multiclass output
   - Sigmoid for binary output

4. **Optimizers:**
   - Adam is good default choice
   - SGD with momentum for some cases
   - Experiment with different optimizers

5. **Monitoring:**
   - Track training and validation loss
   - Watch for overfitting (validation loss increases while training loss decreases)
   - Use early stopping if needed

### CNN-Specific Tips

1. **Filter Sizes:** Start with 3×3, experiment with 5×5
2. **Number of Filters:** Start with 32, increase to 64, 128, etc.
3. **Pooling:** Use 2×2 max pooling
4. **Architecture:** Follow proven architectures (LeNet, AlexNet, etc.)

## Limitations and Assumptions

### Stated Limitations

1. **Cursory Introduction:** Chapter meant as modest step toward mastery
2. **Requires Additional Study:** Many topics need deeper exploration
3. **Assumes Prior Knowledge:** Assumes familiarity with Keras from previous chapters
4. **Limited Coverage:** Does not cover all deep learning architectures

### Assumptions

1. **Supervised Learning:** Most examples assume labeled data
2. **Sufficient Data:** Assumes adequate training data
3. **Computational Resources:** Assumes access to appropriate hardware
4. **Preprocessing:** Assumes data is properly preprocessed

## Related Techniques and References

### Related Chapters

- **Chapter 3:** Classifiers in Machine Learning (activation functions, logistic regression)
- **Chapter 5:** Deep Learning: RNNs and LSTMs (sequential architectures)
- **Chapter 6:** NLP and Reinforcement Learning (applications)

### Key References

**Architectures:**
- CNNs: https://en.wikipedia.org/wiki/Convolutional_neural_network
- MLPs: Standard feed-forward networks
- RNNs: https://en.wikipedia.org/wiki/Recurrent_neural_network

**Initialization:**
- http://www.deeplearning.ai/ai-notes/initialization/

**Bias in AI:**
- https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix

**Audio Signals:**
- https://en.wikipedia.org/wiki/Audio_signal

## Practical Applications

### Use Cases

1. **Image Classification:** MNIST, CIFAR-10, ImageNet
2. **Object Detection:** Identify and locate objects in images
3. **Speech Recognition:** Convert speech to text
4. **Natural Language Processing:** Text classification, generation
5. **Audio Analysis:** Sound classification, music generation
6. **Video Analysis:** Action recognition, object tracking

### Application Domains

- Computer vision
- Natural language processing
- Speech recognition
- Audio processing
- Healthcare (medical image analysis)
- Autonomous vehicles
- Robotics

## Implementation Checklist

### Prerequisites

- [ ] Understanding of basic machine learning concepts
- [ ] Familiarity with Python
- [ ] Knowledge of NumPy, TensorFlow 2, Keras
- [ ] Understanding of activation functions (Chapter 3)

### Setup Steps

1. [ ] Install TensorFlow 2:
   ```bash
   pip install tensorflow
   ```

2. [ ] Install additional libraries:
   ```bash
   pip install numpy matplotlib
   ```

3. [ ] Verify installation:
   ```python
   import tensorflow as tf
   print(tf.__version__)
   ```

### Implementation Steps

1. [ ] Choose architecture (MLP vs CNN)
2. [ ] Define model using Sequential or Functional API
3. [ ] Add layers with appropriate activation functions
4. [ ] Compile model with optimizer, loss, and metrics
5. [ ] Prepare and preprocess data
6. [ ] Train model with fit()
7. [ ] Evaluate on test set
8. [ ] Tune hyperparameters if needed

### For CNNs

1. [ ] Reshape images for Conv2D (add channel dimension)
2. [ ] Add Conv2D layers with filters
3. [ ] Add MaxPooling2D layers
4. [ ] Flatten before fully connected layers
5. [ ] Use softmax for classification output

## Summary

This chapter covered:
- Deep learning fundamentals and architectures
- Perceptrons as building blocks
- MLPs (Multilayer Perceptrons) for fully connected networks
- CNNs (Convolutional Neural Networks) for image processing
- Hyperparameters and their roles
- Forward and backward propagation
- Code examples with Keras/TensorFlow 2
- MNIST dataset examples (MLP and CNN)
- Challenges and limitations of deep learning

The chapter provides practical, implementable techniques for building deep learning models with complete code examples that can be directly used in projects.
