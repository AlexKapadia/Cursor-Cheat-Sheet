---
alwaysApply: true
---
---
alwaysApply: false
---

# Deep Agents - Comprehensive AI Agent Development Guide

## Paper Metadata
- **Title:** Deep Agents Documentation - Building Agents with Planning, File Systems, and Subagents
- **Source:** LangChain Deep Agents Official Documentation
- **Version:** deepagents v1.x
- **Language:** Python 3.10+
- **Documentation URL:** https://docs.langchain.com/oss/python/deepagents/
- **Keywords:** Deep Agents, AI Agents, Planning, File Systems, Subagents, Context Management, LangGraph, Agent Harness, Middleware, Backends, Long-term Memory, Human-in-the-Loop
- **Library:** [`deepagents`](https://pypi.org/project/deepagents/)

## Abstract / Summary

Deep Agents is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.

Deep agents are designed as an "agent harness" - providing the same core tool calling loop as other agent frameworks, but with built-in tools and capabilities that enable agents to:

- **Plan complex tasks** using a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges
- **Manage large amounts of context** through file system tools (`ls`, `read_file`, `write_file`, `edit_file`, `glob`, `grep`) that allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results
- **Delegate work** to specialized subagents for context isolation, keeping the main agent's context clean while still going deep on specific subtasks
- **Persist memory** across conversations and threads using LangGraph's Store, enabling agents to save and retrieve information from previous conversations

## Problem Statement

### Problem Definition

Building agents that can handle complex, multi-step tasks presents several challenges:

1. **Context Window Overflow:** Tools that return variable-length results (web search, RAG) can quickly fill context windows, making it impossible to continue work
2. **Complex Task Planning:** Breaking down complex tasks into manageable steps and tracking progress requires sophisticated planning capabilities
3. **Context Bloat:** Intermediate tool results and conversation history can clutter the agent's context, reducing effectiveness
4. **Memory Management:** Agents need both short-term (conversation state) and long-term (cross-conversation) memory
5. **Task Delegation:** Some tasks require specialized handling or context isolation without cluttering the main agent

### Motivation

Traditional agent frameworks provide tool calling but lack:
- Built-in planning capabilities
- File system abstractions for context management
- Automatic handling of large tool results
- Subagent spawning for context isolation
- Long-term memory persistence

Deep agents address these by providing an "agent harness" - a complete framework with built-in capabilities that handle these common challenges automatically.

### Challenges

1. **Context Engineering:** Managing what information agents see, when they see it, and how it's presented
2. **Large Tool Results:** Handling tool results that exceed context window limits
3. **Task Decomposition:** Breaking complex tasks into manageable, trackable steps
4. **Context Isolation:** Keeping main agent context clean while handling complex subtasks
5. **Memory Persistence:** Maintaining information across conversations and threads

### Scope

Deep agents focuses on:
- **Planning and Task Decomposition:** Built-in `write_todos` tool for task planning
- **File System Operations:** Complete filesystem abstraction with multiple backends
- **Subagent Spawning:** Delegating work to specialized or general-purpose subagents
- **Context Management:** Automatic eviction of large tool results to filesystem
- **Long-term Memory:** Persistent storage across threads using LangGraph Store
- **Human-in-the-Loop:** Approval workflows for sensitive operations
- **Middleware Architecture:** Composable middleware for custom behavior

### Assumptions

- Python 3.10+ is available
- Access to at least one LLM provider (Anthropic Claude Sonnet 4.5 by default, but configurable)
- Understanding of LangGraph and LangChain concepts
- Familiarity with agent development patterns

## Key Concepts and Techniques

### Core Concepts

1. **Agent Harness:** A framework that provides built-in tools and capabilities on top of the core tool calling loop
2. **Planning Tool (`write_todos`):** Tool that enables agents to break down complex tasks into discrete steps and track progress
3. **File System Tools:** Six tools (`ls`, `read_file`, `write_file`, `edit_file`, `glob`, `grep`) for interacting with agent filesystem
4. **Subagents:** Ephemeral agents spawned by the main agent for context isolation and specialized tasks
5. **Backends:** Pluggable storage backends (StateBackend, FilesystemBackend, StoreBackend, CompositeBackend) for filesystem operations
6. **Middleware:** Composable components (TodoListMiddleware, FilesystemMiddleware, SubAgentMiddleware) that add capabilities
7. **Context Eviction:** Automatic dumping of large tool results (>20,000 tokens by default) to filesystem
8. **Long-term Memory:** Persistent storage using LangGraph Store for cross-thread persistence
9. **Human-in-the-Loop:** Interrupt capabilities for tool approval workflows
10. **Conversation Summarization:** Automatic compression of old conversation history when token usage becomes excessive

### Architecture Patterns

1. **Agent Harness Pattern:** Core tool calling loop enhanced with built-in capabilities
2. **Backend Abstraction:** Filesystem operations abstracted behind BackendProtocol
3. **Composite Backend Routing:** Different paths routed to different backends (e.g., `/memories/` → StoreBackend)
4. **Subagent Delegation:** Main agent delegates complex tasks to subagents for context isolation
5. **Middleware Composition:** Multiple middleware components composed together
6. **Context Quarantine:** Using subagents to isolate context and prevent bloat

## Installation and Setup

### Basic Installation

```bash
# Install deepagents
pip install deepagents

# Or with uv
uv add deepagents

# Or with poetry
poetry add deepagents
```

### Quick Start Dependencies

```bash
# For the quickstart example with web search
pip install deepagents tavily-python
```

### API Key Setup

```bash
# Set Anthropic API key (default model)
export ANTHROPIC_API_KEY="your-api-key"

# Or for OpenAI
export OPENAI_API_KEY="your-api-key"

# For web search (optional)
export TAVILY_API_KEY="your-tavily-api-key"
```

### Environment Variables

Deep agents supports multiple model providers. Set the appropriate API key:

- **Anthropic (default):** `ANTHROPIC_API_KEY`
- **OpenAI:** `OPENAI_API_KEY`
- **Google:** `GOOGLE_API_KEY`
- **Other providers:** As required by LangChain model initialization

## Core Capabilities

### 1. Planning and Task Decomposition

Deep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.

**How it works:**
- Agent uses `write_todos` tool to create and update task lists
- Tasks have statuses: `pending`, `in_progress`, `completed`
- Tasks are persisted in agent state
- Agent can adapt plans as new information emerges

**Example:**
```python
# Agent automatically uses write_todos internally
# You can see the todos in the agent's state
agent.invoke({
    "messages": [{"role": "user", "content": "Research quantum computing and write a report"}]
})
# Agent will:
# 1. Use write_todos to break down the task
# 2. Execute steps
# 3. Update todos as progress is made
```

### 2. Context Management with File System Tools

File system tools allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.

**Available Tools:**
- `ls` - List files in a directory with metadata (size, modified time)
- `read_file` - Read file contents with line numbers, supports offset/limit for large files
- `write_file` - Create new files
- `edit_file` - Perform exact string replacements in files (with global replace mode)
- `glob` - Find files matching patterns (e.g., `**/*.py`)
- `grep` - Search file contents with multiple output modes (files only, content with context, or counts)

**Large Tool Result Eviction:**
- Monitors tool call results for size (default threshold: 20,000 tokens)
- When exceeded, writes the result to a file instead
- Replaces the tool result with a concise reference to the file
- Agent can later read the file if needed

### 3. Subagent Spawning

A built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent's context clean while still going deep on specific subtasks.

**Why it's useful:**
- **Context isolation** - Subagent's work doesn't clutter main agent's context
- **Parallel execution** - Multiple subagents can run concurrently
- **Specialization** - Subagents can have different tools/configurations
- **Token efficiency** - Large subtask context is compressed into a single result

**How it works:**
- Main agent has a `task` tool
- When invoked, creates a fresh agent instance with its own context
- Subagent executes autonomously until completion
- Returns a single final report to the main agent
- Subagents are stateless (can't send multiple messages back)

### 4. Long-term Memory

Extend agents with persistent memory across threads using LangGraph's Store. Agents can save and retrieve information from previous conversations.

**Storage Backends:**
- **StateBackend** - Ephemeral in-memory storage (default)
- **FilesystemBackend** - Real filesystem access
- **StoreBackend** - Persistent cross-conversation storage using LangGraph BaseStore
- **CompositeBackend** - Route different paths to different backends

## Quickstart Guide

### Step 1: Install Dependencies

```bash
pip install deepagents tavily-python
```

### Step 2: Set Up API Keys

```bash
export ANTHROPIC_API_KEY="your-api-key"
export TAVILY_API_KEY="your-tavily-api-key"
```

### Step 3: Create a Search Tool

```python
import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )
```

### Step 4: Create a Deep Agent

```python
# System prompt to steer the agent to be an expert researcher
research_instructions = """You are an expert researcher. Your job is to conduct thorough research and then write a polished report.

You have access to an internet search tool as your primary means of gathering information.

## `internet_search`

Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.
"""

agent = create_deep_agent(
    tools=[internet_search],
    system_prompt=research_instructions
)
```

### Step 5: Run the Agent

```python
result = agent.invoke({"messages": [{"role": "user", "content": "What is langgraph?"}]})

# Print the agent's response
print(result["messages"][-1].content)
```

### What Happens Automatically

Your deep agent automatically:

1. **Planned its approach:** Used the built-in `write_todos` tool to break down the research task
2. **Conducted research:** Called the `internet_search` tool to gather information
3. **Managed context:** Used file system tools (`write_file`, `read_file`) to offload large search results
4. **Spawned subagents** (if needed): Delegated complex subtasks to specialized subagents
5. **Synthesized a report:** Compiled findings into a coherent response

## Customization

### Model Configuration

By default, `deepagents` uses `claude-sonnet-4-5-20250929`. You can customize the model:

**Using Model String:**
```python
from langchain.chat_models import init_chat_model
from deepagents import create_deep_agent

model = init_chat_model(model="gpt-5")
agent = create_deep_agent(model=model)
```

**Using LangChain Model Object:**
```python
from langchain_ollama import ChatOllama
from langchain.chat_models import init_chat_model
from deepagents import create_deep_agent

model = init_chat_model(
    model=ChatOllama(
        model="llama3.1",
        temperature=0,
        # other params...
    )
)
agent = create_deep_agent(model=model)
```

### System Prompt Customization

Deep agents come with a built-in system prompt inspired by Claude Code's system prompt. The default system prompt contains detailed instructions for using the built-in planning tool, file system tools, and subagents.

Each deep agent tailored to a use case should include a custom system prompt specific to that use case.

```python
from deepagents import create_deep_agent

research_instructions = """\
You are an expert researcher. Your job is to conduct \
thorough research, and then write a polished report. \
"""

agent = create_deep_agent(
    system_prompt=research_instructions,
)
```

### Tools Configuration

Just like tool-calling agents, a deep agent gets a set of top level tools that it has access to.

```python
import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )

agent = create_deep_agent(
    tools=[internet_search]
)
```

**Default Tools Available:**
- `write_todos` – Update the agent's to-do list
- `ls` – List all files in the agent's filesystem
- `read_file` – Read a file from the agent's filesystem
- `write_file` – Write a new file in the agent's filesystem
- `edit_file` – Edit an existing file in the agent's filesystem
- `task` – Spawn a subagent to handle a specific task

## Agent Harness Capabilities

### File System Access

The harness provides six tools for file system operations, making files first-class citizens in the agent's environment:

| Tool         | Description                                                                                   |
| ------------ | --------------------------------------------------------------------------------------------- |
| `ls`         | List files in a directory with metadata (size, modified time)                                 |
| `read_file`  | Read file contents with line numbers, supports offset/limit for large files                   |
| `write_file` | Create new files                                                                              |
| `edit_file`  | Perform exact string replacements in files (with global replace mode)                         |
| `glob`       | Find files matching patterns (e.g., `**/*.py`)                                                |
| `grep`       | Search file contents with multiple output modes (files only, content with context, or counts) |

### Large Tool Result Eviction

The harness automatically dumps large tool results to the file system when they exceed a token threshold, preventing context window saturation.

**How it works:**
- Monitors tool call results for size (default threshold: 20,000 tokens)
- When exceeded, writes the result to a file instead
- Replaces the tool result with a concise reference to the file
- Agent can later read the file if needed

### Pluggable Storage Backends

The harness abstracts file system operations behind a protocol, allowing different storage strategies for different use cases.

**Available backends:**

1. **StateBackend** - Ephemeral in-memory storage
   - Files live in the agent's state (checkpointed with conversation)
   - Persists within a thread but not across threads
   - Useful for temporary working files

2. **FilesystemBackend** - Real filesystem access
   - Read/write from actual disk
   - Supports virtual mode (sandboxed to a root directory)
   - Integrates with system tools (ripgrep for grep)
   - Security features: path validation, size limits, symlink prevention

3. **StoreBackend** - Persistent cross-conversation storage
   - Uses LangGraph's BaseStore for durability
   - Namespaced per assistant_id
   - Files persist across conversations
   - Useful for long-term memory or knowledge bases

4. **CompositeBackend** - Route different paths to different backends
   - Example: `/` → StateBackend, `/memories/` → StoreBackend
   - Longest-prefix matching for routing
   - Enables hybrid storage strategies

### Task Delegation (Subagents)

The harness allows the main agent to create ephemeral "subagents" for isolated multi-step tasks.

**Why it's useful:**
- **Context isolation** - Subagent's work doesn't clutter main agent's context
- **Parallel execution** - Multiple subagents can run concurrently
- **Specialization** - Subagents can have different tools/configurations
- **Token efficiency** - Large subtask context is compressed into a single result

**How it works:**
- Main agent has a `task` tool
- When invoked, creates a fresh agent instance with its own context
- Subagent executes autonomously until completion
- Returns a single final report to the main agent
- Subagents are stateless (can't send multiple messages back)

**Default subagent:**
- "general-purpose" subagent automatically available
- Has filesystem tools by default
- Can be customized with additional tools/middleware

**Custom subagents:**
- Define specialized subagents with specific tools
- Example: code-reviewer, web-researcher, test-runner
- Configure via `subagents` parameter

### Conversation History Summarization

The harness automatically compresses old conversation history when token usage becomes excessive.

**Configuration:**
- Triggers at 170,000 tokens
- Keeps the most recent 6 messages intact
- Older messages are summarized by the model

**Why it's useful:**
- Enables very long conversations without hitting context limits
- Preserves recent context while compressing ancient history
- Transparent to the agent (appears as a special system message)

### Dangling Tool Call Repair

The harness fixes message history when tool calls are interrupted or cancelled before receiving results.

**The problem:**
- Agent requests tool call: "Please run X"
- Tool call is interrupted (user cancels, error, etc.)
- Agent sees tool_call in AIMessage but no corresponding ToolMessage
- This creates an invalid message sequence

**The solution:**
- Detects AIMessages with tool_calls that have no results
- Creates synthetic ToolMessage responses indicating the call was cancelled
- Repairs the message history before agent execution

**Why it's useful:**
- Prevents agent confusion from incomplete message chains
- Gracefully handles interruptions and errors
- Maintains conversation coherence

### To-do List Tracking

The harness provides a `write_todos` tool that agents can use to maintain a structured task list.

**Features:**
- Track multiple tasks with statuses (pending, in_progress, completed)
- Persisted in agent state
- Helps agent organize complex multi-step work
- Useful for long-running tasks and planning

### Human-in-the-Loop

The harness pauses agent execution at specified tool calls to allow human approval/modification.

**Configuration:**
- Map tool names to interrupt configurations
- Example: `{"edit_file": True}` - pause before every edit
- Can provide approval messages or modify tool inputs

**Why it's useful:**
- Safety gates for destructive operations
- User verification before expensive API calls
- Interactive debugging and guidance

### Prompt Caching (Anthropic)

The harness enables Anthropic's prompt caching feature to reduce redundant token processing.

**How it works:**
- Caches portions of the prompt that repeat across turns
- Significantly reduces latency and cost for long system prompts
- Automatically skips for non-Anthropic models

**Why it's useful:**
- System prompts (especially with filesystem docs) can be 5k+ tokens
- These repeat every turn without caching
- Caching provides ~10x speedup and cost reduction

## Backends

### Quickstart Backends

Here are a few pre-built filesystem backends that you can quickly use with your deep agent:

| Built-in backend                                                 | Description                                                                                                                                                                                                                                                                                   |
| ---------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Default](#statebackend-ephemeral)                               | `agent = create_deep_agent()` <br /> Ephemeral in state. The default filesystem backend for an agent is stored in `langgraph` state. Note that this filesystem only persists *for a single thread*.                                                                                           |
| [Local filesystem persistence](#filesystembackend-local-disk)    | `agent = create_deep_agent(backend=FilesystemBackend(root_dir="/Users/nh/Desktop/"))` <br />This gives the deep agent access to your local machine's filesystem. You can specify the root directory that the agent has access to. Note that any provided `root_dir` must be an absolute path. | |
| [Durable store (LangGraph store)](#storebackend-langgraph-store) | `agent = create_deep_agent(backend=lambda rt: StoreBackend(rt))` <br />This gives the agent access to long-term storage that is *persisted across threads*. This is great for storing longer term memories or instructions that are applicable to the agent over multiple executions.         |
| [Composite](#compositebackend-router)                            | Ephemeral by default, `/memories/` persisted. The Composite backend is maximally flexible. You can specify different routes in the filesystem to point towards different backends. See Composite routing below for a ready-to-paste example.                                                  |

### StateBackend (Ephemeral)

```python
# By default we provide a StateBackend
agent = create_deep_agent()

# Under the hood, it looks like
from deepagents.backends import StateBackend

agent = create_deep_agent(
    backend=(lambda rt: StateBackend(rt))   # Note that the tools access State through the runtime.state
)
```

**How it works:**
- Stores files in LangGraph agent state for the current thread.
- Persists across multiple agent turns on the same thread via checkpoints.

**Best for:**
- A scratch pad for the agent to write intermediate results.
- Automatic eviction of large tool outputs which the agent can then read back in piece by piece.

### FilesystemBackend (Local Disk)

```python
from deepagents.backends import FilesystemBackend

agent = create_deep_agent(
    backend=FilesystemBackend(root_dir=".", virtual_mode=True)
)
```

**How it works:**
- Reads/writes real files under a configurable `root_dir`.
- You can optionally set `virtual_mode=True` to sandbox and normalize paths under `root_dir`.
- Uses secure path resolution, prevents unsafe symlink traversal when possible, can use ripgrep for fast `grep`.

**Best for:**
- Local projects on your machine
- CI sandboxes
- Mounted persistent volumes

### StoreBackend (LangGraph Store)

```python
from langgraph.store.memory import InMemoryStore
from deepagents.backends import StoreBackend

agent = create_deep_agent(
    backend=(lambda rt: StoreBackend(rt)),   # Note that the tools access Store through the runtime.store
    store=InMemoryStore()
)
```

**How it works:**
- Stores files in a LangGraph `BaseStore` provided by the runtime, enabling cross‑thread durable storage.

**Best for:**
- When you already run with a configured LangGraph store (for example, Redis, Postgres, or cloud implementations behind `BaseStore`).
- When you're deploying your agent through LangSmith Deployments (a store is automatically provisioned for your agent).

### CompositeBackend (Router)

```python
from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, StateBackend, StoreBackend
from langgraph.store.memory import InMemoryStore

composite_backend = lambda rt: CompositeBackend(
    default=StateBackend(rt),
    routes={
        "/memories/": StoreBackend(rt),
    }
)

agent = create_deep_agent(
    backend=composite_backend,
    store=InMemoryStore()  # Store passed to create_deep_agent, not backend
)
```

**How it works:**
- Routes file operations to different backends based on path prefix.
- Preserves the original path prefixes in listings and search results.

**Best for:**
- When you want to give your agent both ephemeral and cross-thread storage, a CompositeBackend allows you provide both a StateBackend and StoreBackend
- When you have multiple sources of information that you want to provide to your agent as part of a single filesystem.
  - e.g. You have long-term memories stored under /memories/ in one Store and you also have a custom backend that has documentation accessible at /docs/.

### Specifying a Backend

- Pass a backend to `create_deep_agent(backend=...)`. The filesystem middleware uses it for all tooling.
- You can pass either:
  - An instance implementing `BackendProtocol` (for example, `FilesystemBackend(root_dir=".")`), or
  - A factory `BackendFactory = Callable[[ToolRuntime], BackendProtocol]` (for backends that need runtime like `StateBackend` or `StoreBackend`).
- If omitted, the default is `lambda rt: StateBackend(rt)`.

### Routing to Different Backends

Route parts of the namespace to different backends. Commonly used to persist `/memories/*` and keep everything else ephemeral.

```python
from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, StateBackend, FilesystemBackend

composite_backend = lambda rt: CompositeBackend(
    default=StateBackend(rt),
    routes={
        "/memories/": FilesystemBackend(root_dir="/deepagents/myagent", virtual_mode=True),
    },
)

agent = create_deep_agent(backend=composite_backend)
```

**Behavior:**
- `/workspace/plan.md` → StateBackend (ephemeral)
- `/memories/agent.md` → FilesystemBackend under `/deepagents/myagent`
- `ls`, `glob`, `grep` aggregate results and show original path prefixes.

**Notes:**
- Longer prefixes win (for example, route `"/memories/projects/"` can override `"/memories/"`).
- For StoreBackend routing, ensure the agent runtime provides a store (`runtime.store`).

### Using a Virtual Filesystem

Build a custom backend to project a remote or database filesystem (e.g., S3 or Postgres) into the tools namespace.

**Design guidelines:**
- Paths are absolute (`/x/y.txt`). Decide how to map them to your storage keys/rows.
- Implement `ls_info` and `glob_info` efficiently (server-side listing where available, otherwise local filter).
- Return user-readable error strings for missing files or invalid regex patterns.
- For external persistence, set `files_update=None` in results; only in-state backends should return a `files_update` dict.

**S3-style outline:**
```python
from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult
from deepagents.backends.utils import FileInfo, GrepMatch

class S3Backend(BackendProtocol):
    def __init__(self, bucket: str, prefix: str = ""):
        self.bucket = bucket
        self.prefix = prefix.rstrip("/")

    def _key(self, path: str) -> str:
        return f"{self.prefix}{path}"

    def ls_info(self, path: str) -> list[FileInfo]:
        # List objects under _key(path); build FileInfo entries (path, size, modified_at)
        ...

    def read(self, file_path: str, offset: int = 0, limit: int = 2000) -> str:
        # Fetch object; return numbered content or an error string
        ...

    def grep_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str:
        # Optionally filter server‑side; else list and scan content
        ...

    def glob_info(self, pattern: str, path: str = "/") -> list[FileInfo]:
        # Apply glob relative to path across keys
        ...

    def write(self, file_path: str, content: str) -> WriteResult:
        # Enforce create‑only semantics; return WriteResult(path=file_path, files_update=None)
        ...

    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:
        # Read → replace (respect uniqueness vs replace_all) → write → return occurrences
        ...
```

**Postgres-style outline:**
- Table `files(path text primary key, content text, created_at timestamptz, modified_at timestamptz)`
- Map tool operations onto SQL:
  - `ls_info` uses `WHERE path LIKE $1 || '%'`
  - `glob_info` filter in SQL or fetch then apply glob in Python
  - `grep_raw` can fetch candidate rows by extension or last modified time, then scan lines

### Adding Policy Hooks

Enforce enterprise rules by subclassing or wrapping a backend.

**Block writes/edits under selected prefixes (subclass):**
```python
from deepagents.backends.filesystem import FilesystemBackend
from deepagents.backends.protocol import WriteResult, EditResult

class GuardedBackend(FilesystemBackend):
    def __init__(self, *, deny_prefixes: list[str], **kwargs):
        super().__init__(**kwargs)
        self.deny_prefixes = [p if p.endswith("/") else p + "/" for p in deny_prefixes]

    def write(self, file_path: str, content: str) -> WriteResult:
        if any(file_path.startswith(p) for p in self.deny_prefixes):
            return WriteResult(error=f"Writes are not allowed under {file_path}")
        return super().write(file_path, content)

    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:
        if any(file_path.startswith(p) for p in self.deny_prefixes):
            return EditResult(error=f"Edits are not allowed under {file_path}")
        return super().edit(file_path, old_string, new_string, replace_all)
```

**Generic wrapper (works with any backend):**
```python
from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult
from deepagents.backends.utils import FileInfo, GrepMatch

class PolicyWrapper(BackendProtocol):
    def __init__(self, inner: BackendProtocol, deny_prefixes: list[str] | None = None):
        self.inner = inner
        self.deny_prefixes = [p if p.endswith("/") else p + "/" for p in (deny_prefixes or [])]

    def _deny(self, path: str) -> bool:
        return any(path.startswith(p) for p in self.deny_prefixes)

    def ls_info(self, path: str) -> list[FileInfo]:
        return self.inner.ls_info(path)
    def read(self, file_path: str, offset: int = 0, limit: int = 2000) -> str:
        return self.inner.read(file_path, offset=offset, limit=limit)
    def grep_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str:
        return self.inner.grep_raw(pattern, path, glob)
    def glob_info(self, pattern: str, path: str = "/") -> list[FileInfo]:
        return self.inner.glob_info(pattern, path)
    def write(self, file_path: str, content: str) -> WriteResult:
        if self._deny(file_path):
            return WriteResult(error=f"Writes are not allowed under {file_path}")
        return self.inner.write(file_path, content)
    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:
        if self._deny(file_path):
            return EditResult(error=f"Edits are not allowed under {file_path}")
        return self.inner.edit(file_path, old_string, new_string, replace_all)
```

### Backend Protocol Reference

Backends must implement the `BackendProtocol`.

**Required endpoints:**
- `ls_info(path: str) -> list[FileInfo]`
  - Return entries with at least `path`. Include `is_dir`, `size`, `modified_at` when available. Sort by `path` for deterministic output.
- `read(file_path: str, offset: int = 0, limit: int = 2000) -> str`
  - Return numbered content. On missing file, return `"Error: File '/x' not found"`.
- `grep_raw(pattern: str, path: Optional[str] = None, glob: Optional[str] = None) -> list[GrepMatch] | str`
  - Return structured matches. For an invalid regex, return a string like `"Invalid regex pattern: ..."` (do not raise).
- `glob_info(pattern: str, path: str = "/") -> list[FileInfo]`
  - Return matched files as `FileInfo` entries (empty list if none).
- `write(file_path: str, content: str) -> WriteResult`
  - Create-only. On conflict, return `WriteResult(error=...)`. On success, set `path` and for state backends set `files_update={...}`; external backends should use `files_update=None`.
- `edit(file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult`
  - Enforce uniqueness of `old_string` unless `replace_all=True`. If not found, return error. Include `occurrences` on success.

**Supporting types:**
- `WriteResult(error, path, files_update)`
- `EditResult(error, path, files_update, occurrences)`
- `FileInfo` with fields: `path` (required), optionally `is_dir`, `size`, `modified_at`.
- `GrepMatch` with fields: `path`, `line`, `text`.

## Subagents

### Why Use Subagents?

Subagents solve the **context bloat problem**. When agents use tools with large outputs (web search, file reads, database queries), the context window fills up quickly with intermediate results. Subagents isolate this detailed work—the main agent receives only the final result, not the dozens of tool calls that produced it.

**When to use subagents:**
- ✅ Multi-step tasks that would clutter the main agent's context
- ✅ Specialized domains that need custom instructions or tools
- ✅ Tasks requiring different model capabilities
- ✅ When you want to keep the main agent focused on high-level coordination

**When NOT to use subagents:**
- ❌ Simple, single-step tasks
- ❌ When you need to maintain intermediate context
- ❌ When the overhead outweighs benefits

### Configuration

`subagents` should be a list of dictionaries or `CompiledSubAgent` objects. There are two types:

#### SubAgent (Dictionary-based)

For most use cases, define subagents as dictionaries:

**Required fields:**
- **name** (`str`): Unique identifier for the subagent. The main agent uses this name when calling the `task()` tool.
- **description** (`str`): What this subagent does. Be specific and action-oriented. The main agent uses this to decide when to delegate.
- **system_prompt** (`str`): Instructions for the subagent. Include tool usage guidance and output format requirements.
- **tools** (`List[Callable]`): Tools the subagent can use. Keep this minimal and include only what's needed.

**Optional fields:**
- **model** (`str | BaseChatModel`): Override the main agent's model. Use the format `"provider:model-name"` (for example, `"openai:gpt-4o"`).
- **middleware** (`List[Middleware]`): Additional middleware for custom behavior, logging, or rate limiting.
- **interrupt_on** (`Dict[str, bool]`): Configure human-in-the-loop for specific tools. Requires a checkpointer.

#### CompiledSubAgent

For complex workflows, use a pre-built LangGraph graph:

**Fields:**
- **name** (`str`): Unique identifier
- **description** (`str`): What this subagent does
- **runnable** (`Runnable`): A compiled LangGraph graph (must call `.compile()` first)

### Using SubAgent

```python
import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )

research_subagent = {
    "name": "research-agent",
    "description": "Used to research more in depth questions",
    "system_prompt": "You are a great researcher",
    "tools": [internet_search],
    "model": "openai:gpt-4o",  # Optional override, defaults to main agent model
}
subagents = [research_subagent]

agent = create_deep_agent(
    model="claude-sonnet-4-5-20250929",
    subagents=subagents
)
```

### Using CompiledSubAgent

For more complex use cases, you can provide your own pre-built LangGraph graph as a subagent:

```python
from deepagents import create_deep_agent, CompiledSubAgent
from langchain.agents import create_agent

# Create a custom agent graph
custom_graph = create_agent(
    model=your_model,
    tools=specialized_tools,
    prompt="You are a specialized agent for data analysis..."
)

# Use it as a custom subagent
custom_subagent = CompiledSubAgent(
    name="data-analyzer",
    description="Specialized agent for complex data analysis tasks",
    runnable=custom_graph
)

subagents = [custom_subagent]

agent = create_deep_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[internet_search],
    system_prompt=research_instructions,
    subagents=subagents
)
```

### The General-Purpose Subagent

In addition to any user-defined subagents, deep agents have access to a `general-purpose` subagent at all times. This subagent:

- Has the same system prompt as the main agent
- Has access to all the same tools
- Uses the same model (unless overridden)

**When to use it:**
The general-purpose subagent is ideal for context isolation without specialized behavior. The main agent can delegate a complex multi-step task to this subagent and get a concise result back without bloat from intermediate tool calls.

**Example:**
Instead of the main agent making 10 web searches and filling its context with results, it delegates to the general-purpose subagent: `task(name="general-purpose", task="Research quantum computing trends")`. The subagent performs all the searches internally and returns only a summary.

### Best Practices

#### Write Clear Descriptions

The main agent uses descriptions to decide which subagent to call. Be specific:

✅ **Good:** `"Analyzes financial data and generates investment insights with confidence scores"`

❌ **Bad:** `"Does finance stuff"`

#### Keep System Prompts Detailed

Include specific guidance on how to use tools and format outputs:

```python
research_subagent = {
    "name": "research-agent",
    "description": "Conducts in-depth research using web search and synthesizes findings",
    "system_prompt": """You are a thorough researcher. Your job is to:

    1. Break down the research question into searchable queries
    2. Use internet_search to find relevant information
    3. Synthesize findings into a comprehensive but concise summary
    4. Cite sources when making claims

    Output format:
    - Summary (2-3 paragraphs)
    - Key findings (bullet points)
    - Sources (with URLs)

    Keep your response under 500 words to maintain clean context.""",
    "tools": [internet_search],
}
```

#### Minimize Tool Sets

Only give subagents the tools they need. This improves focus and security:

```python
# ✅ Good: Focused tool set
email_agent = {
    "name": "email-sender",
    "tools": [send_email, validate_email],  # Only email-related
}

# ❌ Bad: Too many tools
email_agent = {
    "name": "email-sender",
    "tools": [send_email, web_search, database_query, file_upload],  # Unfocused
}
```

#### Choose Models by Task

Different models excel at different tasks:

```python
subagents = [
    {
        "name": "contract-reviewer",
        "description": "Reviews legal documents and contracts",
        "system_prompt": "You are an expert legal reviewer...",
        "tools": [read_document, analyze_contract],
        "model": "claude-sonnet-4-5-20250929",  # Large context for long documents
    },
    {
        "name": "financial-analyst",
        "description": "Analyzes financial data and market trends",
        "system_prompt": "You are an expert financial analyst...",
        "tools": [get_stock_price, analyze_fundamentals],
        "model": "openai:gpt-5",  # Better for numerical analysis
    },
]
```

#### Return Concise Results

Instruct subagents to return summaries, not raw data:

```python
data_analyst = {
    "system_prompt": """Analyze the data and return:
    1. Key insights (3-5 bullet points)
    2. Overall confidence score
    3. Recommended next actions

    Do NOT include:
    - Raw data
    - Intermediate calculations
    - Detailed tool outputs

    Keep response under 300 words."""
}
```

### Common Patterns

#### Multiple Specialized Subagents

Create specialized subagents for different domains:

```python
from deepagents import create_deep_agent

subagents = [
    {
        "name": "data-collector",
        "description": "Gathers raw data from various sources",
        "system_prompt": "Collect comprehensive data on the topic",
        "tools": [web_search, api_call, database_query],
    },
    {
        "name": "data-analyzer",
        "description": "Analyzes collected data for insights",
        "system_prompt": "Analyze data and extract key insights",
        "tools": [statistical_analysis],
    },
    {
        "name": "report-writer",
        "description": "Writes polished reports from analysis",
        "system_prompt": "Create professional reports from insights",
        "tools": [format_document],
    },
]

agent = create_deep_agent(
    model="claude-sonnet-4-5-20250929",
    system_prompt="You coordinate data analysis and reporting. Use subagents for specialized tasks.",
    subagents=subagents
)
```

**Workflow:**
1. Main agent creates high-level plan
2. Delegates data collection to data-collector
3. Passes results to data-analyzer
4. Sends insights to report-writer
5. Compiles final output

Each subagent works with clean context focused only on its task.

### Troubleshooting

#### Subagent Not Being Called

**Problem:** Main agent tries to do work itself instead of delegating.

**Solutions:**

1. **Make descriptions more specific:**
   ```python
   # ✅ Good
   {"name": "research-specialist", "description": "Conducts in-depth research on specific topics using web search. Use when you need detailed information that requires multiple searches."}

   # ❌ Bad
   {"name": "helper", "description": "helps with stuff"}
   ```

2. **Instruct main agent to delegate:**
   ```python
   agent = create_deep_agent(
       system_prompt="""...your instructions...

       IMPORTANT: For complex tasks, delegate to your subagents using the task() tool.
       This keeps your context clean and improves results.""",
       subagents=[...]
   )
   ```

#### Context Still Getting Bloated

**Problem:** Context fills up despite using subagents.

**Solutions:**

1. **Instruct subagent to return concise results:**
   ```python
   system_prompt="""...

   IMPORTANT: Return only the essential summary.
   Do NOT include raw data, intermediate search results, or detailed tool outputs.
   Your response should be under 500 words."""
   ```

2. **Use filesystem for large data:**
   ```python
   system_prompt="""When you gather large amounts of data:
   1. Save raw data to /data/raw_results.txt
   2. Process and analyze the data
   3. Return only the analysis summary

   This keeps context clean."""
   ```

#### Wrong Subagent Being Selected

**Problem:** Main agent calls inappropriate subagent for the task.

**Solution:** Differentiate subagents clearly in descriptions:

```python
subagents = [
    {
        "name": "quick-researcher",
        "description": "For simple, quick research questions that need 1-2 searches. Use when you need basic facts or definitions.",
    },
    {
        "name": "deep-researcher",
        "description": "For complex, in-depth research requiring multiple searches, synthesis, and analysis. Use for comprehensive reports.",
    }
]
```

## Human-in-the-Loop

### Basic Configuration

The `interrupt_on` parameter accepts a dictionary mapping tool names to interrupt configurations. Each tool can be configured with:

- **`True`**: Enable interrupts with default behavior (approve, edit, reject allowed)
- **`False`**: Disable interrupts for this tool
- **`{"allowed_decisions": [...]}`**: Custom configuration with specific allowed decisions

```python
from langchain.tools import tool
from deepagents import create_deep_agent
from langgraph.checkpoint.memory import MemorySaver

@tool
def delete_file(path: str) -> str:
    """Delete a file from the filesystem."""
    return f"Deleted {path}"

@tool
def read_file(path: str) -> str:
    """Read a file from the filesystem."""
    return f"Contents of {path}"

@tool
def send_email(to: str, subject: str, body: str) -> str:
    """Send an email."""
    return f"Sent email to {to}"

# Checkpointer is REQUIRED for human-in-the-loop
checkpointer = MemorySaver()

agent = create_deep_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[delete_file, read_file, send_email],
    interrupt_on={
        "delete_file": True,  # Default: approve, edit, reject
        "read_file": False,   # No interrupts needed
        "send_email": {"allowed_decisions": ["approve", "reject"]},  # No editing
    },
    checkpointer=checkpointer  # Required!
)
```

### Decision Types

The `allowed_decisions` list controls what actions a human can take when reviewing a tool call:

- **`"approve"`**: Execute the tool with the original arguments as proposed by the agent
- **`"edit"`**: Modify the tool arguments before execution
- **`"reject"`**: Skip executing this tool call entirely

You can customize which decisions are available for each tool:

```python
interrupt_on = {
    # Sensitive operations: allow all options
    "delete_file": {"allowed_decisions": ["approve", "edit", "reject"]},

    # Moderate risk: approval or rejection only
    "write_file": {"allowed_decisions": ["approve", "reject"]},

    # Must approve (no rejection allowed)
    "critical_operation": {"allowed_decisions": ["approve"]},
}
```

### Handling Interrupts

When an interrupt is triggered, the agent pauses execution and returns control. Check for interrupts in the result and handle them accordingly.

```python
import uuid
from langgraph.types import Command

# Create config with thread_id for state persistence
config = {"configurable": {"thread_id": str(uuid.uuid4())}}

# Invoke the agent
result = agent.invoke({
    "messages": [{"role": "user", "content": "Delete the file temp.txt"}]
}, config=config)

# Check if execution was interrupted
if result.get("__interrupt__"):
    # Extract interrupt information
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]
    review_configs = interrupts["review_configs"]

    # Create a lookup map from tool name to review config
    config_map = {cfg["action_name"]: cfg for cfg in review_configs}

    # Display the pending actions to the user
    for action in action_requests:
        review_config = config_map[action["name"]]
        print(f"Tool: {action['name']}")
        print(f"Arguments: {action['args']}")
        print(f"Allowed decisions: {review_config['allowed_decisions']}")

    # Get user decisions (one per action_request, in order)
    decisions = [
        {"type": "approve"}  # User approved the deletion
    ]

    # Resume execution with decisions
    result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config  # Must use the same config!
    )

# Process final result
print(result["messages"][-1].content)
```

### Multiple Tool Calls

When the agent calls multiple tools that require approval, all interrupts are batched together in a single interrupt. You must provide decisions for each one in order.

```python
config = {"configurable": {"thread_id": str(uuid.uuid4())}}

result = agent.invoke({
    "messages": [{
        "role": "user",
        "content": "Delete temp.txt and send an email to admin@example.com"
    }]
}, config=config)

if result.get("__interrupt__"):
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]

    # Two tools need approval
    assert len(action_requests) == 2

    # Provide decisions in the same order as action_requests
    decisions = [
        {"type": "approve"},  # First tool: delete_file
        {"type": "reject"}    # Second tool: send_email
    ]

    result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config
    )
```

### Editing Tool Arguments

When `"edit"` is in the allowed decisions, you can modify the tool arguments before execution:

```python
if result.get("__interrupt__"):
    interrupts = result["__interrupt__"][0].value
    action_request = interrupts["action_requests"][0]

    # Original args from the agent
    print(action_request["args"])  # {"to": "everyone@company.com", ...}

    # User decides to edit the recipient
    decisions = [{
        "type": "edit",
        "edited_action": {
            "name": action_request["name"],  # Must include the tool name
            "args": {"to": "team@company.com", "subject": "...", "body": "..."}
        }
    }]

    result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config
    )
```

### Subagent Interrupts

Each subagent can have its own `interrupt_on` configuration that overrides the main agent's settings:

```python
agent = create_deep_agent(
    tools=[delete_file, read_file],
    interrupt_on={
        "delete_file": True,
        "read_file": False,
    },
    subagents=[{
        "name": "file-manager",
        "description": "Manages file operations",
        "system_prompt": "You are a file management assistant.",
        "tools": [delete_file, read_file],
        "interrupt_on": {
            # Override: require approval for reads in this subagent
            "delete_file": True,
            "read_file": True,  # Different from main agent!
        }
    }],
    checkpointer=checkpointer
)
```

When a subagent triggers an interrupt, the handling is the same – check for `__interrupt__` and resume with `Command`.

### Best Practices

#### Always Use a Checkpointer

Human-in-the-loop requires a checkpointer to persist agent state between the interrupt and resume:

```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
agent = create_deep_agent(
    tools=[...],
    interrupt_on={...},
    checkpointer=checkpointer  # Required for HITL
)
```

#### Use the Same Thread ID

When resuming, you must use the same config with the same `thread_id`:

```python
# First call
config = {"configurable": {"thread_id": "my-thread"}}
result = agent.invoke(input, config=config)

# Resume (use same config)
result = agent.invoke(Command(resume={...}), config=config)
```

#### Match Decision Order to Actions

The decisions list must match the order of `action_requests`:

```python
if result.get("__interrupt__"):
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]

    # Create one decision per action, in order
    decisions = []
    for action in action_requests:
        decision = get_user_decision(action)  # Your logic
        decisions.append(decision)

    result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config
    )
```

#### Tailor Configurations by Risk

Configure different tools based on their risk level:

```python
interrupt_on = {
    # High risk: full control (approve, edit, reject)
    "delete_file": {"allowed_decisions": ["approve", "edit", "reject"]},
    "send_email": {"allowed_decisions": ["approve", "edit", "reject"]},

    # Medium risk: no editing allowed
    "write_file": {"allowed_decisions": ["approve", "reject"]},

    # Low risk: no interrupts
    "read_file": False,
    "list_files": False,
}
```

## Long-term Memory

### Setup

Configure long-term memory by using a `CompositeBackend` that routes the `/memories/` path to a `StoreBackend`:

```python
from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, StateBackend, StoreBackend
from langgraph.store.memory import InMemoryStore

def make_backend(runtime):
    return CompositeBackend(
        default=StateBackend(runtime),  # Ephemeral storage
        routes={
            "/memories/": StoreBackend(runtime)  # Persistent storage
        }
    )

agent = create_deep_agent(
    store=InMemoryStore(),  # Required for StoreBackend
    backend=make_backend
)
```

### How It Works

When using `CompositeBackend`, deep agents maintain **two separate filesystems**:

#### 1. Short-term (Transient) Filesystem

- Stored in the agent's state (via `StateBackend`)
- Persists only within a single thread
- Files are lost when the thread ends
- Accessed through standard paths: `/notes.txt`, `/workspace/draft.md`

#### 2. Long-term (Persistent) Filesystem

- Stored in a LangGraph Store (via `StoreBackend`)
- Persists across all threads and conversations
- Survives agent restarts
- Accessed through paths prefixed with `/memories/`: `/memories/preferences.txt`

#### Path Routing

The `CompositeBackend` routes file operations based on path prefixes:

- Files with paths starting with `/memories/` are stored in the Store (persistent)
- Files without this prefix remain in transient state
- All filesystem tools (`ls`, `read_file`, `write_file`, `edit_file`) work with both

```python
# Transient file (lost after thread ends)
agent.invoke({
    "messages": [{"role": "user", "content": "Write draft to /draft.txt"}]
})

# Persistent file (survives across threads)
agent.invoke({
    "messages": [{"role": "user", "content": "Save final report to /memories/report.txt"}]
})
```

### Cross-thread Persistence

Files in `/memories/` can be accessed from any thread:

```python
import uuid

# Thread 1: Write to long-term memory
config1 = {"configurable": {"thread_id": str(uuid.uuid4())}}
agent.invoke({
    "messages": [{"role": "user", "content": "Save my preferences to /memories/preferences.txt"}]
}, config=config1)

# Thread 2: Read from long-term memory (different conversation!)
config2 = {"configurable": {"thread_id": str(uuid.uuid4())}}
agent.invoke({
    "messages": [{"role": "user", "content": "What are my preferences?"}]
}, config=config2)
# Agent can read /memories/preferences.txt from the first thread
```

### Use Cases

#### User Preferences

Store user preferences that persist across sessions:

```python
agent = create_deep_agent(
    store=InMemoryStore(),
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    ),
    system_prompt="""When users tell you their preferences, save them to
    /memories/user_preferences.txt so you remember them in future conversations."""
)
```

#### Self-improving Instructions

An agent can update its own instructions based on feedback:

```python
agent = create_deep_agent(
    store=InMemoryStore(),
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    ),
    system_prompt="""You have a file at /memories/instructions.txt with additional
    instructions and preferences.

    Read this file at the start of conversations to understand user preferences.

    When users provide feedback like "please always do X" or "I prefer Y",
    update /memories/instructions.txt using the edit_file tool."""
)
```

Over time, the instructions file accumulates user preferences, helping the agent improve.

#### Knowledge Base

Build up knowledge over multiple conversations:

```python
# Conversation 1: Learn about a project
agent.invoke({
    "messages": [{"role": "user", "content": "We're building a web app with React. Save project notes."}]
})

# Conversation 2: Use that knowledge
agent.invoke({
    "messages": [{"role": "user", "content": "What framework are we using?"}]
})
# Agent reads /memories/project_notes.txt from previous conversation
```

#### Research Projects

Maintain research state across sessions:

```python
research_agent = create_deep_agent(
    store=InMemoryStore(),
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    ),
    system_prompt="""You are a research assistant.

    Save your research progress to /memories/research/:
    - /memories/research/sources.txt - List of sources found
    - /memories/research/notes.txt - Key findings and notes
    - /memories/research/report.md - Final report draft

    This allows research to continue across multiple sessions."""
)
```

### Store Implementations

Any LangGraph `BaseStore` implementation works:

#### InMemoryStore (Development)

Good for testing and development, but data is lost on restart:

```python
from langgraph.store.memory import InMemoryStore

store = InMemoryStore()
agent = create_deep_agent(
    store=store,
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    )
)
```

#### PostgresStore (Production)

For production, use a persistent store:

```python
from langgraph.store.postgres import PostgresStore
import os

store = PostgresStore(connection_string=os.environ["DATABASE_URL"])
agent = create_deep_agent(
    store=store,
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    )
)
```

### Best Practices

#### Use Descriptive Paths

Organize persistent files with clear paths:

```
/memories/user_preferences.txt
/memories/research/topic_a/sources.txt
/memories/research/topic_a/notes.txt
/memories/project/requirements.md
```

#### Document the Memory Structure

Tell the agent what's stored where in your system prompt:

```
Your persistent memory structure:
- /memories/preferences.txt: User preferences and settings
- /memories/context/: Long-term context about the user
- /memories/knowledge/: Facts and information learned over time
```

#### Prune Old Data

Implement periodic cleanup of outdated persistent files to keep storage manageable.

#### Choose the Right Storage

- **Development:** Use `InMemoryStore` for quick iteration
- **Production:** Use `PostgresStore` or other persistent stores
- **Multi-tenant:** Consider using assistant_id-based namespacing in your store

## Middleware Architecture

Deep agents are built with a modular middleware architecture. Deep agents have access to:

1. A planning tool
2. A filesystem for storing context and long-term memories
3. The ability to spawn subagents

Each feature is implemented as separate middleware. When you create a deep agent with `create_deep_agent`, we automatically attach `TodoListMiddleware`, `FilesystemMiddleware`, and `SubAgentMiddleware` to your agent.

Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently.

### To-do List Middleware

Planning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in.

`TodoListMiddleware` provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the `write_todos` tool to keep track of what it's doing and what still needs to be done.

```python
from langchain.agents import create_agent
from langchain.agents.middleware import TodoListMiddleware

# TodoListMiddleware is included by default in create_deep_agent
# You can customize it if building a custom agent
agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    # Custom planning instructions can be added via middleware
    middleware=[
        TodoListMiddleware(
            system_prompt="Use the write_todos tool to..."  # Optional: Custom addition to the system prompt
        ),
    ],
)
```

### Filesystem Middleware

Context engineering is a main challenge in building effective agents. This is particularly difficult when using tools that return variable-length results (for example, web_search and rag), as long tool results can quickly fill your context window.

`FilesystemMiddleware` provides four tools for interacting with both short-term and long-term memory:

* **ls**: List the files in the filesystem
* **read_file**: Read an entire file or a certain number of lines from a file
* **write_file**: Write a new file to the filesystem
* **edit_file**: Edit an existing file in the filesystem

```python
from langchain.agents import create_agent
from deepagents.middleware.filesystem import FilesystemMiddleware

# FilesystemMiddleware is included by default in create_deep_agent
# You can customize it if building a custom agent
agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    middleware=[
        FilesystemMiddleware(
            backend=None,  # Optional: custom backend (defaults to StateBackend)
            system_prompt="Write to the filesystem when...",  # Optional custom addition to the system prompt
            custom_tool_descriptions={
                "ls": "Use the ls tool when...",
                "read_file": "Use the read_file tool to..."
            }  # Optional: Custom descriptions for filesystem tools
        ),
    ],
)
```

#### Short-term vs. Long-term Filesystem

By default, these tools write to a local "filesystem" in your graph state. To enable persistent storage across threads, configure a `CompositeBackend` that routes specific paths (like `/memories/`) to a `StoreBackend`.

```python
from langchain.agents import create_agent
from deepagents.middleware import FilesystemMiddleware
from deepagents.backends import CompositeBackend, StateBackend, StoreBackend
from langgraph.store.memory import InMemoryStore

store = InMemoryStore()

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    store=store,
    middleware=[
        FilesystemMiddleware(
            backend=lambda rt: CompositeBackend(
                default=StateBackend(rt),
                routes={"/memories/": StoreBackend(rt)}
            ),
            custom_tool_descriptions={
                "ls": "Use the ls tool when...",
                "read_file": "Use the read_file tool to..."
            }  # Optional: Custom descriptions for filesystem tools
        ),
    ],
)
```

When you configure a `CompositeBackend` with a `StoreBackend` for `/memories/`, any files prefixed with **/memories/** are saved to persistent storage and survive across different threads. Files without this prefix remain in ephemeral state storage.

### Subagent Middleware

Handing off tasks to subagents isolates context, keeping the main (supervisor) agent's context window clean while still going deep on a task.

The subagents middleware allows you to supply subagents through a `task` tool.

```python
from langchain.tools import tool
from langchain.agents import create_agent
from deepagents.middleware.subagents import SubAgentMiddleware


@tool
def get_weather(city: str) -> str:
    """Get the weather in a city."""
    return f"The weather in {city} is sunny."

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    middleware=[
        SubAgentMiddleware(
            default_model="claude-sonnet-4-5-20250929",
            default_tools=[],
            subagents=[
                {
                    "name": "weather",
                    "description": "This subagent can get weather in cities.",
                    "system_prompt": "Use the get_weather tool to get the weather in a city.",
                    "tools": [get_weather],
                    "model": "gpt-4o",
                    "middleware": [],
                }
            ],
        )
    ],
)
```

A subagent is defined with a **name**, **description**, **system prompt**, and **tools**. You can also provide a subagent with a custom **model**, or with additional **middleware**. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.

For more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.

```python
from langchain.agents import create_agent
from deepagents.middleware.subagents import SubAgentMiddleware
from deepagents import CompiledSubAgent
from langgraph.graph import StateGraph

# Create a custom LangGraph graph
def create_weather_graph():
    workflow = StateGraph(...)
    # Build your custom graph
    return workflow.compile()

weather_graph = create_weather_graph()

# Wrap it in a CompiledSubAgent
weather_subagent = CompiledSubAgent(
    name="weather",
    description="This subagent can get weather in cities.",
    runnable=weather_graph
)

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    middleware=[
        SubAgentMiddleware(
            default_model="claude-sonnet-4-5-20250929",
            default_tools=[],
            subagents=[weather_subagent],
        )
    ],
)
```

In addition to any user-defined subagents, the main agent has access to a `general-purpose` subagent at all times. This subagent has the same instructions as the main agent and all the tools it has access to. The primary purpose of the `general-purpose` subagent is context isolation—the main agent can delegate a complex task to this subagent and get a concise answer back without bloat from intermediate tool calls.

## Deep Agents CLI

### Overview

A terminal interface for building agents with persistent memory. Agents maintain context across sessions, learn project conventions, and execute code with approval controls.

The Deep Agents CLI has the following built-in capabilities:

* **File operations** - read, write, and edit files in your project with tools that enable agents to manage and modify code and documentation.
* **Shell command execution** - execute shell commands to run tests, build projects, manage dependencies, and interact with version control systems.
* **Web search** - search the web for up-to-date information and documentation (requires Tavily API key).
* **HTTP requests** - make HTTP requests to APIs and external services for data fetching and integration tasks.
* **Task planning and tracking** - break down complex tasks into discrete steps and track progress through the built-in todo system.
* **Memory storage and retrieval** - store and retrieve information across sessions, enabling agents to remember project conventions and learned patterns.
* **Human-in-the-loop** - require human approval for sensitive tool operations.

### Quick Start

#### Step 1: Set Your API Key

Export as an environment variable:

```bash
export ANTHROPIC_API_KEY="your-api-key"
```

Or create a `.env` file in your project root:

```bash
ANTHROPIC_API_KEY=your-api-key
```

#### Step 2: Run the CLI

```bash
uvx deepagents-cli
```

#### Step 3: Give the Agent a Task

```bash
> Create a Python script that prints "Hello, World!"
```

The agent proposes changes with diffs for your approval before modifying files.

### Additional Installation and Configuration

**Install locally if needed:**

```bash
# Using pip
pip install deepagents-cli

# Using uv
uv add deepagents-cli
```

**The CLI uses Anthropic Claude Sonnet 4 by default. To use OpenAI:**

```bash
export OPENAI_API_KEY="your-key"
```

**Enable web search (optional):**

```bash
export TAVILY_API_KEY="your-key"
```

API keys can be set as environment variables or in a `.env` file.

### Configuration

#### Command-line Options

| Option                 | Description                                                 |
| ---------------------- | ----------------------------------------------------------- |
| `--agent NAME`         | Use named agent with separate memory                        |
| `--auto-approve`       | Skip tool confirmation prompts (toggle with `Ctrl+T`)       |
| `--sandbox TYPE`       | Execute in remote sandbox: `modal`, `daytona`, or `runloop` |
| `--sandbox-id ID`      | Reuse existing sandbox                                      |
| `--sandbox-setup PATH` | Run setup script in sandbox                                 |

#### CLI Commands

| Command                                         | Description                             |
| ----------------------------------------------- | --------------------------------------- |
| `deepagents list`                               | List all agents                         |
| `deepagents help`                               | Show help                               |
| `deepagents reset --agent NAME`                 | Clear agent memory and reset to default |
| `deepagents reset --agent NAME --target SOURCE` | Copy memory from another agent          |

### Interactive Mode

#### Slash Commands

Use these commands within the CLI session:

* `/tokens` - Display token usage
* `/clear` - Clear conversation history
* `/exit` - Exit the CLI

#### Bash Commands

Execute shell commands directly by prefixing with `!`:

```bash
!git status
!npm test
!ls -la
```

#### Keyboard Shortcuts

| Shortcut    | Action              |
| ----------- | ------------------- |
| `Enter`     | Submit              |
| `Alt+Enter` | Newline             |
| `Ctrl+E`    | External editor     |
| `Ctrl+T`    | Toggle auto-approve |
| `Ctrl+C`    | Interrupt           |
| `Ctrl+D`    | Exit                |

### Setting Project Conventions with Memories

Agents store information in `~/.deepagents/AGENT_NAME/memories/` as markdown files using a memory-first protocol:

1. **Research**: Searches memory for relevant context before starting tasks
2. **Response**: Checks memory when uncertain during execution
3. **Learning**: Automatically saves new information for future sessions

Organize memories by topic with descriptive filenames:

```
~/.deepagents/backend-dev/memories/
├── api-conventions.md
├── database-schema.md
└── deployment-process.md
```

**Teach the agent conventions once:**

```bash
uvx deepagents-cli --agent backend-dev
> Our API uses snake_case and includes created_at/updated_at timestamps
```

**It remembers for future sessions:**

```bash
> Create a /users endpoint
# Applies conventions without prompting
```

### Using Remote Sandboxes

Execute code in isolated remote environments for safety and flexibility. Remote sandboxes provide the following benefits:

* **Safety**: Protect your local machine from potentially harmful code execution
* **Clean environments**: Use specific dependencies or OS configurations without local setup
* **Parallel execution**: Run multiple agents simultaneously in isolated environments
* **Long-running tasks**: Execute time-intensive operations without blocking your machine
* **Reproducibility**: Ensure consistent execution environments across teams

**To use a remote sandbox, follow these steps:**

1. **Configure your sandbox provider** ([Runloop](https://www.runloop.ai/), [Daytona](https://www.daytona.io/), or [Modal](https://modal.com/)):

   ```bash
   # Runloop
   export RUNLOOP_API_KEY="your-key"

   # Daytona
   export DAYTONA_API_KEY="your-key"

   # Modal
   modal setup
   ```

2. **Run the CLI with a sandbox:**

   ```bash
   uvx deepagents-cli --sandbox runloop --sandbox-setup ./setup.sh
   ```

   The agent runs locally but executes all code operations in the remote sandbox. Optional setup scripts can configure environment variables, clone repositories, and prepare dependencies.

3. **(Optional) Create a `setup.sh` file** to configure your sandbox environment:

   ```bash
   #!/bin/bash
   set -e

   # Clone repository using GitHub token
   git clone https://x-access-token:${GITHUB_TOKEN}@github.com/username/repo.git $HOME/workspace
   cd $HOME/workspace

   # Make environment variables persistent
   cat >> ~/.bashrc <<'EOF'
   export GITHUB_TOKEN="${GITHUB_TOKEN}"
   export OPENAI_API_KEY="${OPENAI_API_KEY}"
   cd $HOME/workspace
   EOF

   source ~/.bashrc
   ```

   Store secrets in a local `.env` file for the setup script to access.

**Warning:** Sandboxes isolate code execution, but agents remain vulnerable to prompt injection with untrusted inputs. Use human-in-the-loop approval, short-lived secrets, and trusted setup scripts only. Note that sandbox APIs are evolving rapidly, and we expect more providers to support proxies that help mitigate prompt injection and secrets management concerns.

## Relationship to LangChain Ecosystem

Deep agents is built on top of:

* [LangGraph](/oss/python/langgraph/overview) - Provides the underlying graph execution and state management
* [LangChain](/oss/python/langchain/overview) - Tools and model integrations work seamlessly with deep agents
* [LangSmith](/langsmith/home) - Observability, evaluation, and deployment

Deep agents applications can be deployed via [LangSmith Deployment](/langsmith/deployments) and monitored with [LangSmith Observability](/langsmith/observability).

## Implementation Patterns

### Basic Agent Pattern

```python
from deepagents import create_deep_agent

agent = create_deep_agent(
    tools=[your_tools],
    system_prompt="Your instructions here"
)
```

### Agent with Custom Model

```python
from deepagents import create_deep_agent
from langchain.chat_models import init_chat_model

model = init_chat_model(model="gpt-5")
agent = create_deep_agent(
    model=model,
    tools=[your_tools]
)
```

### Agent with Long-term Memory

```python
from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, StateBackend, StoreBackend
from langgraph.store.memory import InMemoryStore

agent = create_deep_agent(
    store=InMemoryStore(),
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    )
)
```

### Agent with Subagents

```python
from deepagents import create_deep_agent

subagents = [
    {
        "name": "research-agent",
        "description": "Conducts in-depth research",
        "system_prompt": "You are a researcher...",
        "tools": [internet_search],
    }
]

agent = create_deep_agent(
    tools=[your_tools],
    subagents=subagents
)
```

### Agent with Human-in-the-Loop

```python
from deepagents import create_deep_agent
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()

agent = create_deep_agent(
    tools=[sensitive_tools],
    interrupt_on={
        "delete_file": True,
        "send_email": {"allowed_decisions": ["approve", "reject"]},
    },
    checkpointer=checkpointer
)
```

## Best Practices and Recommendations

### Planning Best Practices

1. **Encourage Planning:** Instruct agents to use `write_todos` for complex tasks
2. **Adapt Plans:** Allow agents to update todos as new information emerges
3. **Track Progress:** Use todos to monitor multi-step task completion

### Context Management Best Practices

1. **Use Filesystem for Large Data:** Save large tool results to files instead of keeping in context
2. **Organize Files:** Use clear directory structures (`/workspace/`, `/data/`, `/memories/`)
3. **Read Incrementally:** Use `read_file` with offset/limit for large files
4. **Leverage Eviction:** Let the system automatically evict large results (>20k tokens)

### Subagent Best Practices

1. **Clear Descriptions:** Write specific, action-oriented descriptions
2. **Focused Tools:** Only give subagents the tools they need
3. **Concise Results:** Instruct subagents to return summaries, not raw data
4. **Model Selection:** Choose appropriate models for different tasks
5. **Context Isolation:** Use subagents for tasks that would bloat main context

### Memory Best Practices

1. **Use Descriptive Paths:** Organize persistent files clearly (`/memories/user_preferences.txt`)
2. **Document Structure:** Tell agents what's stored where
3. **Prune Old Data:** Periodically clean up outdated persistent files
4. **Choose Right Storage:** Use InMemoryStore for dev, PostgresStore for production

### Human-in-the-Loop Best Practices

1. **Always Use Checkpointer:** Required for interrupt/resume functionality
2. **Same Thread ID:** Use consistent thread_id for interrupt handling
3. **Match Decision Order:** Decisions must match action_requests order
4. **Tailor by Risk:** Configure different tools based on risk level

## Limitations and Assumptions

### Stated Limitations

1. **Context Window Limits:** Despite eviction and summarization, very long conversations may still hit limits
2. **Subagent Statelessness:** Subagents can't send multiple messages back to main agent
3. **Backend Dependencies:** StoreBackend requires LangGraph Store to be configured
4. **Sandbox Security:** Remote sandboxes isolate execution but don't prevent prompt injection

### Assumptions

1. **Python 3.10+** is available
2. **LLM Provider Access** - Access to at least one model provider
3. **LangGraph/LangChain** - Understanding of underlying frameworks
4. **Thread Management** - Proper thread_id management for persistence

### Constraints

1. **Backend Protocol:** Custom backends must implement BackendProtocol
2. **Thread Persistence:** Long-term memory requires consistent thread_id usage
3. **Checkpointer Required:** Human-in-the-loop requires checkpointer
4. **Model Compatibility:** Some features (prompt caching) only work with Anthropic models

## Related Techniques and References

### Related Techniques

- **LangGraph:** Underlying graph execution framework
- **LangChain Agents:** Standard agent patterns and tool calling
- **Context Engineering:** Techniques for managing agent context
- **RAG (Retrieval-Augmented Generation):** Combining retrieval with generation

### Key References

- [LangGraph Documentation](https://docs.langchain.com/oss/python/langgraph/overview)
- [LangChain Documentation](https://docs.langchain.com/oss/python/langchain/overview)
- [Deep Agents Reference](https://reference.langchain.com/python/deepagents/)
- [Agent Frameworks, Runtimes, and Harnesses](https://blog.langchain.com/agent-frameworks-runtimes-and-harnesses-oh-my/)

## Practical Applications

### Use Cases

1. **Research Agents:** Conduct research and write reports
2. **Code Review Agents:** Review code and provide feedback
3. **Data Analysis Agents:** Analyze data and generate insights
4. **Content Creation Agents:** Create and edit content
5. **Project Management Agents:** Plan and track project tasks

### Application Domains

- **Software Development:** Code generation, review, and documentation
- **Research:** Literature review, data analysis, report writing
- **Content Creation:** Writing, editing, content planning
- **Data Analysis:** Data processing, analysis, visualization
- **Project Management:** Task planning, tracking, coordination

## Implementation Checklist

### Prerequisites

- [ ] Python 3.10+ installed
- [ ] API key for LLM provider (Anthropic, OpenAI, etc.)
- [ ] `deepagents` package installed
- [ ] Understanding of LangGraph/LangChain concepts

### Setup Steps

1. [ ] Install deepagents: `pip install deepagents`
2. [ ] Set API key: `export ANTHROPIC_API_KEY="your-key"`
3. [ ] (Optional) Install additional tools: `pip install tavily-python`
4. [ ] (Optional) Set up persistent store for long-term memory

### Implementation Steps

1. [ ] Define your tools
2. [ ] Create system prompt
3. [ ] Configure backend (if needed)
4. [ ] Set up subagents (if needed)
5. [ ] Configure human-in-the-loop (if needed)
6. [ ] Create agent with `create_deep_agent`
7. [ ] Test with simple tasks
8. [ ] Iterate and refine

### Testing Steps

1. [ ] Test basic agent functionality
2. [ ] Test file system operations
3. [ ] Test subagent delegation
4. [ ] Test long-term memory persistence
5. [ ] Test human-in-the-loop interrupts
6. [ ] Test large tool result eviction
7. [ ] Test conversation summarization
8. [ ] Test planning and todo tracking

## Troubleshooting Guide

### Common Issues

#### Agent Not Using Planning Tool

**Problem:** Agent doesn't break down complex tasks.

**Solution:**
- Explicitly instruct agent to use `write_todos` in system prompt
- Provide examples of task decomposition
- Check that TodoListMiddleware is included

#### Context Window Overflow

**Problem:** Context fills up despite filesystem tools.

**Solution:**
- Ensure large tool results are being evicted (check threshold: 20,000 tokens)
- Use subagents for complex multi-step tasks
- Instruct agent to save large data to files
- Use `read_file` with offset/limit for large files

#### Subagents Not Being Called

**Problem:** Main agent does work itself instead of delegating.

**Solution:**
- Make subagent descriptions more specific and action-oriented
- Explicitly instruct main agent to delegate complex tasks
- Provide clear examples of when to use subagents

#### Long-term Memory Not Persisting

**Problem:** Files in `/memories/` are lost between sessions.

**Solution:**
- Ensure StoreBackend is configured correctly
- Verify store is passed to `create_deep_agent`
- Check that CompositeBackend routes `/memories/` to StoreBackend
- Verify thread_id is consistent across sessions

#### Human-in-the-Loop Not Working

**Problem:** Interrupts not triggering.

**Solution:**
- Ensure checkpointer is provided
- Verify `interrupt_on` configuration is correct
- Check that thread_id is used consistently
- Verify tool names match exactly

#### Backend Errors

**Problem:** File operations failing.

**Solution:**
- Verify backend protocol implementation
- Check path validation and security settings
- Ensure proper error handling in custom backends
- Verify backend factory function signature

### Performance Optimization

1. **Use Prompt Caching:** Enable for Anthropic models to reduce latency and cost
2. **Optimize Tool Results:** Keep tool outputs concise when possible
3. **Use Subagents Strategically:** Delegate only when context isolation is needed
4. **Leverage Eviction:** Let system automatically handle large results
5. **Monitor Token Usage:** Track usage to identify optimization opportunities

## Advanced Topics

### Custom Middleware

You can create custom middleware to extend agent capabilities:

```python
from langchain.agents.middleware import Middleware

class CustomMiddleware(Middleware):
    def before_agent(self, state, config):
        # Modify state before agent execution
        return state
    
    def after_agent(self, state, config):
        # Process state after agent execution
        return state
```

### Custom Backend Implementation

For advanced use cases, implement a custom backend:

```python
from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult
from deepagents.backends.utils import FileInfo, GrepMatch

class CustomBackend(BackendProtocol):
    def ls_info(self, path: str) -> list[FileInfo]:
        # Implement listing
        pass
    
    def read(self, file_path: str, offset: int = 0, limit: int = 2000) -> str:
        # Implement reading
        pass
    
    def write(self, file_path: str, content: str) -> WriteResult:
        # Implement writing
        pass
    
    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:
        # Implement editing
        pass
    
    def grep_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str:
        # Implement grep
        pass
    
    def glob_info(self, pattern: str, path: str = "/") -> list[FileInfo]:
        # Implement glob
        pass
```

### Integration with LangSmith

Deep agents integrate seamlessly with LangSmith for observability:

```python
from langsmith import traceable

@traceable
def my_agent_function():
    agent = create_deep_agent(...)
    return agent.invoke(...)
```

### Deployment Considerations

1. **Store Configuration:** Use persistent stores (PostgresStore) in production
2. **Checkpointer:** Use durable checkpointers for production
3. **Error Handling:** Implement robust error handling
4. **Monitoring:** Set up LangSmith monitoring
5. **Security:** Review backend security settings
6. **Scaling:** Consider thread management for high concurrency

## Summary

Deep Agents provides a comprehensive framework for building agents that can:

- **Plan complex tasks** using built-in todo tracking
- **Manage context** through filesystem abstraction
- **Delegate work** to specialized subagents
- **Persist memory** across conversations
- **Handle interruptions** for human approval
- **Optimize performance** through automatic eviction and summarization

The framework is built on LangGraph and LangChain, providing a solid foundation while adding essential capabilities for production-ready agent development.

## Quick Reference

### Key Functions

- `create_deep_agent()` - Create a deep agent with all capabilities
- `StateBackend()` - Ephemeral in-memory storage
- `FilesystemBackend()` - Real filesystem access
- `StoreBackend()` - Persistent cross-conversation storage
- `CompositeBackend()` - Route paths to different backends
- `CompiledSubAgent()` - Use pre-built graphs as subagents

### Key Tools (Built-in)

- `write_todos` - Plan and track tasks
- `ls` - List files
- `read_file` - Read file contents
- `write_file` - Create files
- `edit_file` - Edit files
- `glob` - Find files by pattern
- `grep` - Search file contents
- `task` - Spawn subagents

### Key Parameters

- `model` - LLM model to use
- `tools` - List of tools for agent
- `system_prompt` - Agent instructions
- `backend` - Filesystem backend
- `subagents` - List of subagent definitions
- `interrupt_on` - Human-in-the-loop configuration
- `checkpointer` - State persistence
- `store` - LangGraph store for long-term memory

---

**END OF DEEP AGENTS COMPREHENSIVE GUIDE**